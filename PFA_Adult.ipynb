{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRgVkCt5xaan",
    "outputId": "67d86d41-def8-4860-db76-87e05121c5a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: catboost in /opt/conda/lib/python3.11/site-packages (1.2.7)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.11/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from catboost) (3.9.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.11/site-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.11/site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from catboost) (1.14.1)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.11/site-packages (from catboost) (6.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.11/site-packages (from plotly->catboost) (1.34.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: shap in /opt/conda/lib/python3.11/site-packages (0.47.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from shap) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from shap) (1.6.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /opt/conda/lib/python3.11/site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in /opt/conda/lib/python3.11/site-packages (from shap) (24.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in /opt/conda/lib/python3.11/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /opt/conda/lib/python3.11/site-packages (from shap) (0.61.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.11/site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from shap) (4.13.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: plotly in /opt/conda/lib/python3.11/site-packages (6.0.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.11/site-packages (from plotly) (1.34.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from plotly) (24.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gdown in /opt/conda/lib/python3.11/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from gdown) (4.13.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: PrettyTable in /opt/conda/lib/python3.11/site-packages (3.16.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from PrettyTable) (0.2.13)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gdown in /opt/conda/lib/python3.11/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from gdown) (4.13.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install catboost\n",
    "!pip install shap\n",
    "!pip install plotly\n",
    "!pip install gdown\n",
    "!pip install PrettyTable\n",
    "!pip install seaborn\n",
    "!pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cy0HIlItxeGt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gdown\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkRY5XY0225t"
   },
   "source": [
    "# **Fairness Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dMHfad_Hxgro"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from prettytable import PrettyTable\n",
    "from itertools import combinations\n",
    "\n",
    "def fair_Demographic_Parity(X,y_var,sensitive_var,prediction):\n",
    "#   print(\"----------------------------------------------------\")\n",
    "#   print(\"\\t\\t\\t\",sensitive_var)\n",
    "#   print('----------------------------------------------------')\n",
    "  groups = X[sensitive_var].unique()\n",
    "  positive_proportions = []\n",
    "\n",
    "  # Create a PrettyTable object\n",
    "  table = PrettyTable()\n",
    "  table.field_names = [\"Name\", \"proportion_positive\"]\n",
    "\n",
    "  for group in groups:\n",
    "    group_data = X[X[sensitive_var] == group]\n",
    "\n",
    "    y_pred_group = group_data[prediction]\n",
    "    # Calculate Proportion Positive for the current group\n",
    "    proportion_positive = y_pred_group.mean()\n",
    "\n",
    "    # Append Proportion Positive for the current group\n",
    "    positive_proportions.append(proportion_positive)\n",
    "    table.add_row([group, proportion_positive])\n",
    "    # Find the maximum absolute difference in Proportion Positive across groups\n",
    "  max_diff = max(abs(pp1 - pp2) for pp1, pp2 in combinations(positive_proportions, 2))\n",
    "\n",
    "#   print(\"Demographic Parity Score :\",max_diff)\n",
    "#   print(table)\n",
    "  return max_diff\n",
    "\n",
    "def fair_Eq_Odds(X,y_var,sensitive_var,prediction):\n",
    "#   print(\"----------------------------------------------------\")\n",
    "#   print(\"\\t\\t\\t\",sensitive_var)\n",
    "#   print('----------------------------------------------------')\n",
    "  groups = X[sensitive_var].unique()\n",
    "  equal_odds_scores = []\n",
    "\n",
    "  # Create a PrettyTable object\n",
    "  table = PrettyTable()\n",
    "  table.field_names = [\"Name\", \"TPR\", \"FPR\"]\n",
    "\n",
    "  for group in groups:\n",
    "    group_data = X[X[sensitive_var] == group]\n",
    "    y_true_group = group_data[y_var]\n",
    "    y_pred_group = group_data[prediction]\n",
    "\n",
    "    cm = confusion_matrix(y_true_group, y_pred_group)\n",
    "\n",
    "    # Calculate Sensitivity (True Positive Rate)\n",
    "    tpr = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # Calculate Specificity (True Negative Rate)\n",
    "    fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    # Calculate Equal Odds for the current group\n",
    "    equal_odds_scores.append((tpr, fpr))\n",
    "    # Add data to the table\n",
    "    table.add_row([group, tpr, fpr])\n",
    "\n",
    "  # Find the maximum absolute difference in TPR and FPR across groups\n",
    "  max_diff = max(abs(tpr1 - tpr2) + abs(fpr1 - fpr2) for (tpr1, fpr1),\n",
    "   (tpr2, fpr2) in zip(equal_odds_scores[::2], equal_odds_scores[1::2]))\n",
    "\n",
    "#   print(\"Equalized Odds Score :\",max_diff)\n",
    "#   print(table)\n",
    "  return max_diff\n",
    "\n",
    "\n",
    "\n",
    "def fair_Eq_Opportunity(X,y_var,sensitive_var,prediction):\n",
    "#   print(\"----------------------------------------------------\")\n",
    "#   print(\"\\t\\t\\t\",sensitive_var)\n",
    "#   print('----------------------------------------------------')\n",
    "  groups = X[sensitive_var].unique()\n",
    "  equal_odds_scores = []\n",
    "\n",
    "  # Create a PrettyTable object\n",
    "  table = PrettyTable()\n",
    "  table.field_names = [\"Name\", \"TPR\"]\n",
    "\n",
    "  for group in groups:\n",
    "    group_data = X[X[sensitive_var] == group]\n",
    "    y_true_group = group_data[y_var]\n",
    "    y_pred_group = group_data[prediction]\n",
    "\n",
    "    cm = confusion_matrix(y_true_group, y_pred_group)\n",
    "\n",
    "    # Calculate Sensitivity (True Positive Rate)\n",
    "    tpr = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # Calculate Specificity (True Negative Rate)\n",
    "    fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    # Calculate Equal Odds for the current group\n",
    "    equal_odds_scores.append((tpr, fpr))\n",
    "    # Add data to the table\n",
    "    table.add_row([group, tpr])\n",
    "\n",
    "  # Find the maximum absolute difference in TPR and FPR across groups\n",
    "  max_diff = max(abs(tpr1 - tpr2)  for (tpr1, fpr1),\n",
    "   (tpr2, fpr2) in zip(equal_odds_scores[::2], equal_odds_scores[1::2]))\n",
    "\n",
    "#   print(\"Equal Opportunity Score :\",max_diff)\n",
    "#   print(table)\n",
    "  return max_diff\n",
    "\n",
    "def fair_Treatment_Equality (X,y_var,sensitive_var,prediction):\n",
    "#   print(\"----------------------------------------------------\")\n",
    "#   print(\"\\t\\t\\t\",sensitive_var)\n",
    "#   print('----------------------------------------------------')\n",
    "  groups = X[sensitive_var].unique()\n",
    "  tpr_scores = []\n",
    "  fpr_scores = []\n",
    "\n",
    "  # Create a PrettyTable object\n",
    "  table = PrettyTable()\n",
    "  table.field_names = [\"Name\", \"TPR\",'FPR']\n",
    "\n",
    "  for group in groups:\n",
    "    group_data = X[X[sensitive_var] == group]\n",
    "    y_true_group = group_data[y_var]\n",
    "    y_pred_group = group_data[prediction]\n",
    "\n",
    "    # Calculate True Positive Rate (Sensitivity)\n",
    "    cm = confusion_matrix(y_true_group, y_pred_group)\n",
    "    tpr = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # Calculate False Positive Rate\n",
    "    fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    # Append TPR and FPR for the current group\n",
    "    tpr_scores.append(tpr)\n",
    "    fpr_scores.append(fpr)\n",
    "\n",
    "    table.add_row([group, tpr,fpr])\n",
    "\n",
    "  # Find the maximum absolute difference in Proportion Positive across groups\n",
    "  max_diff_tpr = max(abs(tpr1 - tpr2) for tpr1, tpr2 in combinations(tpr_scores, 2))\n",
    "  max_diff_fpr = max(abs(fpr1 - fpr2) for fpr1, fpr2 in combinations(fpr_scores, 2))\n",
    "\n",
    "\n",
    "#   print(\"Treatment Equality Score :\",max(max_diff_tpr, max_diff_fpr))\n",
    "\n",
    "#   print(table)\n",
    "  return max(max_diff_tpr, max_diff_fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBANabZz3AsS"
   },
   "source": [
    "# **Equal sampling function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wjtOkXih2-kX"
   },
   "outputs": [],
   "source": [
    "def equal_sampling(dataframe, variables):\n",
    "    \"\"\"\n",
    "    Perform equal sampling from a pandas DataFrame based on a list of variables.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pandas DataFrame.\n",
    "    - variables: list of column names in the dataframe to be used for creating groups.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame with equal representation from each group formed by the unique combinations of the specified variables.\n",
    "    \"\"\"\n",
    "    # Generate all unique combinations of the specified variables\n",
    "    combinations = dataframe.groupby(variables).size().reset_index().rename(columns={0: 'count'})\n",
    "\n",
    "    # Determine the smallest group size among these combinations\n",
    "    min_size = combinations['count'].min()\n",
    "\n",
    "    # Initialize an empty DataFrame to store the sampled data\n",
    "    sampled_df = pd.DataFrame(columns=dataframe.columns)\n",
    "\n",
    "    # Loop through each unique combination, filter the dataframe, and sample\n",
    "    for _, row in combinations.iterrows():\n",
    "        filter_criteria = (dataframe[variables[0]] == row[variables[0]])\n",
    "        for var in variables[1:]:\n",
    "            filter_criteria &= (dataframe[var] == row[var])\n",
    "\n",
    "        sampled_group = dataframe[filter_criteria].sample(n=min_size, replace=True) # Use replace=True if min_size is larger than the group\n",
    "        sampled_df = pd.concat([sampled_df, sampled_group])\n",
    "\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JmgEhMo3YuC"
   },
   "source": [
    "# **Import the dataset and read**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class\n",
      "0  <=50K\n",
      "1  <=50K\n",
      "2   >50K\n",
      "3   >50K\n",
      "4  <=50K\n",
      "      sex\n",
      "0    Male\n",
      "1    Male\n",
      "2    Male\n",
      "3    Male\n",
      "4  Female\n",
      "DataFrame shape: (48842, 54)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>occupation_Protective-serv</th>\n",
       "      <th>occupation_Sales</th>\n",
       "      <th>occupation_Tech-support</th>\n",
       "      <th>occupation_Transport-moving</th>\n",
       "      <th>occupation_nan</th>\n",
       "      <th>relationship_Not-in-family</th>\n",
       "      <th>relationship_Other-relative</th>\n",
       "      <th>relationship_Own-child</th>\n",
       "      <th>relationship_Unmarried</th>\n",
       "      <th>relationship_Wife</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>226802</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>89814</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>336951</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>160323</td>\n",
       "      <td>10</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>103497</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>27</td>\n",
       "      <td>257302</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>40</td>\n",
       "      <td>154374</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>58</td>\n",
       "      <td>151910</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>22</td>\n",
       "      <td>201490</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>52</td>\n",
       "      <td>287927</td>\n",
       "      <td>9</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0       25  226802              7             0             0              40   \n",
       "1       38   89814              9             0             0              50   \n",
       "2       28  336951             12             0             0              40   \n",
       "3       44  160323             10          7688             0              40   \n",
       "4       18  103497             10             0             0              30   \n",
       "...    ...     ...            ...           ...           ...             ...   \n",
       "48837   27  257302             12             0             0              38   \n",
       "48838   40  154374              9             0             0              40   \n",
       "48839   58  151910              9             0             0              40   \n",
       "48840   22  201490              9             0             0              20   \n",
       "48841   52  287927              9         15024             0              40   \n",
       "\n",
       "       workclass_Local-gov  workclass_Never-worked  workclass_Private  \\\n",
       "0                      0.0                     0.0                1.0   \n",
       "1                      0.0                     0.0                1.0   \n",
       "2                      1.0                     0.0                0.0   \n",
       "3                      0.0                     0.0                1.0   \n",
       "4                      0.0                     0.0                0.0   \n",
       "...                    ...                     ...                ...   \n",
       "48837                  0.0                     0.0                1.0   \n",
       "48838                  0.0                     0.0                1.0   \n",
       "48839                  0.0                     0.0                1.0   \n",
       "48840                  0.0                     0.0                1.0   \n",
       "48841                  0.0                     0.0                0.0   \n",
       "\n",
       "       workclass_Self-emp-inc  ...  occupation_Protective-serv  \\\n",
       "0                         0.0  ...                         0.0   \n",
       "1                         0.0  ...                         0.0   \n",
       "2                         0.0  ...                         1.0   \n",
       "3                         0.0  ...                         0.0   \n",
       "4                         0.0  ...                         0.0   \n",
       "...                       ...  ...                         ...   \n",
       "48837                     0.0  ...                         0.0   \n",
       "48838                     0.0  ...                         0.0   \n",
       "48839                     0.0  ...                         0.0   \n",
       "48840                     0.0  ...                         0.0   \n",
       "48841                     1.0  ...                         0.0   \n",
       "\n",
       "       occupation_Sales  occupation_Tech-support  occupation_Transport-moving  \\\n",
       "0                   0.0                      0.0                          0.0   \n",
       "1                   0.0                      0.0                          0.0   \n",
       "2                   0.0                      0.0                          0.0   \n",
       "3                   0.0                      0.0                          0.0   \n",
       "4                   0.0                      0.0                          0.0   \n",
       "...                 ...                      ...                          ...   \n",
       "48837               0.0                      1.0                          0.0   \n",
       "48838               0.0                      0.0                          0.0   \n",
       "48839               0.0                      0.0                          0.0   \n",
       "48840               0.0                      0.0                          0.0   \n",
       "48841               0.0                      0.0                          0.0   \n",
       "\n",
       "       occupation_nan  relationship_Not-in-family  \\\n",
       "0                 0.0                         0.0   \n",
       "1                 0.0                         0.0   \n",
       "2                 0.0                         0.0   \n",
       "3                 0.0                         0.0   \n",
       "4                 1.0                         0.0   \n",
       "...               ...                         ...   \n",
       "48837             0.0                         0.0   \n",
       "48838             0.0                         0.0   \n",
       "48839             0.0                         0.0   \n",
       "48840             0.0                         0.0   \n",
       "48841             0.0                         0.0   \n",
       "\n",
       "       relationship_Other-relative  relationship_Own-child  \\\n",
       "0                              0.0                     1.0   \n",
       "1                              0.0                     0.0   \n",
       "2                              0.0                     0.0   \n",
       "3                              0.0                     0.0   \n",
       "4                              0.0                     1.0   \n",
       "...                            ...                     ...   \n",
       "48837                          0.0                     0.0   \n",
       "48838                          0.0                     0.0   \n",
       "48839                          0.0                     0.0   \n",
       "48840                          0.0                     1.0   \n",
       "48841                          0.0                     0.0   \n",
       "\n",
       "       relationship_Unmarried  relationship_Wife  \n",
       "0                         0.0                0.0  \n",
       "1                         0.0                0.0  \n",
       "2                         0.0                0.0  \n",
       "3                         0.0                0.0  \n",
       "4                         0.0                0.0  \n",
       "...                       ...                ...  \n",
       "48837                     0.0                1.0  \n",
       "48838                     0.0                0.0  \n",
       "48839                     1.0                0.0  \n",
       "48840                     0.0                0.0  \n",
       "48841                     0.0                1.0  \n",
       "\n",
       "[48842 rows x 54 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Initialize the OneHotEncoder\n",
    "# encoder = OneHotEncoder(drop='first')  # drop='first' to avoid multicollinearity\n",
    "# encoded_cols = encoder.fit_transform(df[categorical_cols_more_than_two]).toarray()  # Converts the sparse matrix output to a dense array\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def load_and_preprocess_adult_dataset():\n",
    "    # Fetch the dataset\n",
    "    data = fetch_openml(name='adult', version=2, as_frame=True)\n",
    "    df = data.frame\n",
    "    \n",
    "    # Separate and store 'sex' column\n",
    "    sex_df = df.loc[:,['sex']]\n",
    "    \n",
    "    # Separate the target column\n",
    "    y_df = df.loc[:,['class']]\n",
    "    \n",
    "    # Drop the columns not needed for feature encoding\n",
    "    df = df.drop(['sex', 'race', 'native-country', 'class'], axis=1)\n",
    "    \n",
    "    # Identify categorical columns with more than 2 unique values\n",
    "    categorical_cols = df.select_dtypes(include=['category']).columns\n",
    "    categorical_cols_more_than_two = [col for col in categorical_cols if len(df[col].unique()) > 2]\n",
    "    \n",
    "    # One-hot encode these columns\n",
    "    encoder = OneHotEncoder(drop='first')  # Automatically handles multicollinearity by dropping the first category\n",
    "    encoded_cols = encoder.fit_transform(df[categorical_cols_more_than_two]).toarray()\n",
    "    \n",
    "    # Create a DataFrame with the encoded columns\n",
    "    encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols_more_than_two))\n",
    "    \n",
    "    # Drop original categorical columns from df and join with encoded_df\n",
    "    df = df.drop(categorical_cols_more_than_two, axis=1)\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    \n",
    "    return df, y_df, sex_df\n",
    "\n",
    "# Example usage\n",
    "df_adult, df_y, df_sen = load_and_preprocess_adult_dataset()\n",
    "\n",
    "print(df_y.head())      # Print the first few rows of the target DataFrame\n",
    "print(df_sen.head())      # Print the first few rows of the target DataFrame\n",
    "print(\"DataFrame shape:\", df_adult.shape)\n",
    "initial_full_dataset = df_adult\n",
    "initial_full_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "9hPYTYpFxkIu",
    "outputId": "f549e792-5981-4c1d-8a46-62c88d7c45d6"
   },
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/ImeshEkanayake/Fair_Datasets/main/Adult%20Census%20Income.csv\"\n",
    "# initial_full_dataset = pd.read_csv(url)\n",
    "# initial_full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_full_dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOOTQiwQ3fsL"
   },
   "source": [
    "# **Assign variable categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YRAOd5C7xnNx"
   },
   "outputs": [],
   "source": [
    "# X_var_con = ['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week']\n",
    "# X_var_cat = ['workclass', 'marital-status',  'occupation','relationship','native-country']\n",
    "\n",
    "# Sen_var = ['sex'] #'race',\n",
    "# y_var = ['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydOM4hqbe1W-"
   },
   "source": [
    "# **One-Hot Encoding on Categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ReTA8XSi2zMq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sex'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sen.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IpuOPV2F2zvc"
   },
   "outputs": [],
   "source": [
    "\n",
    "replacement_mapping = { '>50K': 1, '<=50K': 0}\n",
    "df_y['class'] = df_y['class'].map(replacement_mapping).to_numpy()\n",
    "df_y = pd.DataFrame(df_y)\n",
    "\n",
    "\n",
    "replacement_mapping = { 'Male': 1, 'Female': 0}\n",
    "df_sen['sex'] = df_sen['sex'].map(replacement_mapping).to_numpy()\n",
    "df_sen = pd.DataFrame(df_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_6H_qNxe7hd"
   },
   "source": [
    "# **Edit this cell acording to the varibale names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A1qLUJfWe8bV"
   },
   "outputs": [],
   "source": [
    "ohe_full_dataset = df_adult\n",
    "ohe_full_dataset[\"Sensitive\"] = df_sen['sex']\n",
    "ohe_full_dataset[\"Class\"] = df_y['class']\n",
    "df_X = df_adult\n",
    "# median_value = ohe_full_dataset['Class_con'].median()\n",
    "# ohe_full_dataset[\"Class\"] = (ohe_full_dataset['Class_con'] >= median_value).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Id6nuVjo45JL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Dataset_name</th>\n",
       "      <th>Alpha-pred</th>\n",
       "      <th>Beta-Fair</th>\n",
       "      <th>Gamma-privacy</th>\n",
       "      <th>Balance_Class</th>\n",
       "      <th>Balance_Sensitive</th>\n",
       "      <th>Fairness_cost_function</th>\n",
       "      <th>Dataset_training_size</th>\n",
       "      <th>Dataset_testing_size</th>\n",
       "      <th>...</th>\n",
       "      <th>Fairness_Demographic_Parity_DNN_prediction_Training</th>\n",
       "      <th>Fairness_Demographic_Parity_DNN_prediction_Testing</th>\n",
       "      <th>Fairness_Demographic_Parity_After_CatBoost_prediction_Training</th>\n",
       "      <th>Fairness_Demographic_Parity_After_CatBoost_prediction_Testing</th>\n",
       "      <th>Fairness_Treatment_Equality_Before_CatBoost_prediction_Training</th>\n",
       "      <th>Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing</th>\n",
       "      <th>Fairness_Treatment_Equality_DNN_prediction_Training</th>\n",
       "      <th>Fairness_Treatment_Equality_DNN_prediction_Testing</th>\n",
       "      <th>Fairness_Treatment_Equality_After_CatBoost_prediction_Training</th>\n",
       "      <th>Fairness_Treatment_Equality_After_CatBoost_prediction_Testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Data, Dataset_name, Alpha-pred, Beta-Fair, Gamma-privacy, Balance_Class, Balance_Sensitive, Fairness_cost_function, Dataset_training_size, Dataset_testing_size, Test_loss_predictor, Test_loss_Discriminator_fool, Test_total_loss_gen, Train_loss_predictor, Train_loss_Discriminator_fool, Train_total_loss_gen, Dataset_fairness_Demographic_Parity, Report_DNN_Discriminator_Training_Accuracy, Report_DNN_Discriminator_Training_precision, Report_DNN_Discriminator_Training_recall, Report_DNN_Discriminator_Training_f1, Report_DNN_Discriminator_Testing_Accuracy, Report_DNN_Discriminator_Testing_precision, Report_DNN_Discriminator_Testing_recall, Report_DNN_Discriminator_Testing_f1, Report_Predictor_Training_Accuracy, Report_Predictor_Training_precision, Report_Predictor_Training_recall, Report_Predictor_Training_f1, Report_Predictor_Testing_Accuracy, Report_Predictor_Testing_precision, Report_Predictor_Testing_recall, Report_Predictor_Testing_f1, Report_Before_CatBoost_Prediction_Training_Accuracy, Report_Before_CatBoost_Prediction_Training_precision, Report_Before_CatBoost_Prediction_Training_recall, Report_Before_CatBoost_Prediction_Training_f1, Report_Before_CatBoost_Prediction_Testing_Accuracy, Report_Before_CatBoost_Prediction_Testing_precision, Report_Before_CatBoost_Prediction_Testing_recall, Report_Before_CatBoost_Prediction_Testing_f1, Report_After_CatBoost_Prediction_Training_Accuracy, Report_After_CatBoost_Prediction_Training_precision, Report_After_CatBoost_Prediction_Training_recall, Report_After_CatBoost_Prediction_Training_f1, Report_After_CatBoost_Prediction_Testing_Accuracy, Report_After_CatBoost_Prediction_Testing_precision, Report_After_CatBoost_Prediction_Testing_recall, Report_After_CatBoost_Prediction_Testing_f1, Report_Before_CatBoost_Sensitive_Training_Accuracy, Report_Before_CatBoost_Sensitive_Training_precision, Report_Before_CatBoost_Sensitive_Training_recall, Report_Before_CatBoost_Sensitive_Training_f1, Report_Before_CatBoost_Sensitive_Testing_Accuracy, Report_Before_CatBoost_Sensitive_Testing_precision, Report_Before_CatBoost_Sensitive_Testing_recall, Report_Before_CatBoost_Sensitive_Testing_f1, Report_After_CatBoost_Sensitive_Training_Accuracy, Report_After_CatBoost_Sensitive_Training_precision, Report_After_CatBoost_Sensitive_Training_recall, Report_After_CatBoost_Sensitive_Training_f1, Report_After_CatBoost_Sensitive_Testing_Accuracy, Report_After_CatBoost_Sensitive_Testing_precision, Report_After_CatBoost_Sensitive_Testing_recall, Report_After_CatBoost_Sensitive_Testing_f1, Fairness_Equal_Odd_Before_CatBoost_prediction_Training, Fairness_Equal_Odd_Before_CatBoost_prediction_Testing, Fairness_Equal_Odd_DNN_prediction_Training, Fairness_Equal_Odd_DNN_prediction_Testing, Fairness_Equal_Odd_After_CatBoost_prediction_Training, Fairness_Equal_Odd_After_CatBoost_prediction_Testing, Fairness_Equal_Opportunity_Before_CatBoost_prediction_Training, Fairness_Equal_Opportunity_Before_CatBoost_prediction_Testing, Fairness_Equal_Opportunity_DNN_prediction_Training, Fairness_Equal_Opportunity_DNN_prediction_Testing, Fairness_Equal_Opportunity_After_CatBoost_prediction_Training, Fairness_Equal_Opportunity_After_CatBoost_prediction_Testing, Fairness_Demographic_Parity_Before_CatBoost_prediction_Training, Fairness_Demographic_Parity_Before_CatBoost_prediction_Testing, Fairness_Demographic_Parity_DNN_prediction_Training, Fairness_Demographic_Parity_DNN_prediction_Testing, Fairness_Demographic_Parity_After_CatBoost_prediction_Training, Fairness_Demographic_Parity_After_CatBoost_prediction_Testing, Fairness_Treatment_Equality_Before_CatBoost_prediction_Training, Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing, Fairness_Treatment_Equality_DNN_prediction_Training, Fairness_Treatment_Equality_DNN_prediction_Testing, Fairness_Treatment_Equality_After_CatBoost_prediction_Training, Fairness_Treatment_Equality_After_CatBoost_prediction_Testing]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 89 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(columns=['Data', 'Dataset_name',\n",
    "        \"Alpha-pred\",\n",
    "        \"Beta-Fair\",\n",
    "        \"Gamma-privacy\",\n",
    "         'Balance_Class', 'Balance_Sensitive',\n",
    "       'Fairness_cost_function', 'Dataset_training_size',\n",
    "       'Dataset_testing_size', \n",
    "        \"Test_loss_predictor\",\n",
    "        \"Test_loss_Discriminator_fool\",\n",
    "        \"Test_total_loss_gen\",\n",
    "        \"Train_loss_predictor\",\n",
    "        \"Train_loss_Discriminator_fool\",\n",
    "        \"Train_total_loss_gen\",\n",
    "       'Dataset_fairness_Demographic_Parity',\n",
    "       'Report_DNN_Discriminator_Training_Accuracy',\n",
    "       'Report_DNN_Discriminator_Training_precision',\n",
    "       'Report_DNN_Discriminator_Training_recall',\n",
    "       'Report_DNN_Discriminator_Training_f1',\n",
    "       'Report_DNN_Discriminator_Testing_Accuracy',\n",
    "       'Report_DNN_Discriminator_Testing_precision',\n",
    "       'Report_DNN_Discriminator_Testing_recall',\n",
    "       'Report_DNN_Discriminator_Testing_f1',\n",
    "       'Report_Predictor_Training_Accuracy',\n",
    "       'Report_Predictor_Training_precision',\n",
    "       'Report_Predictor_Training_recall', 'Report_Predictor_Training_f1',\n",
    "       'Report_Predictor_Testing_Accuracy',\n",
    "       'Report_Predictor_Testing_precision', 'Report_Predictor_Testing_recall',\n",
    "       'Report_Predictor_Testing_f1',\n",
    "       'Report_Before_CatBoost_Prediction_Training_Accuracy',\n",
    "       'Report_Before_CatBoost_Prediction_Training_precision',\n",
    "       'Report_Before_CatBoost_Prediction_Training_recall',\n",
    "       'Report_Before_CatBoost_Prediction_Training_f1',\n",
    "       'Report_Before_CatBoost_Prediction_Testing_Accuracy',\n",
    "       'Report_Before_CatBoost_Prediction_Testing_precision',\n",
    "       'Report_Before_CatBoost_Prediction_Testing_recall',\n",
    "       'Report_Before_CatBoost_Prediction_Testing_f1',\n",
    "       'Report_After_CatBoost_Prediction_Training_Accuracy',\n",
    "       'Report_After_CatBoost_Prediction_Training_precision',\n",
    "       'Report_After_CatBoost_Prediction_Training_recall',\n",
    "       'Report_After_CatBoost_Prediction_Training_f1',\n",
    "       'Report_After_CatBoost_Prediction_Testing_Accuracy',\n",
    "       'Report_After_CatBoost_Prediction_Testing_precision',\n",
    "       'Report_After_CatBoost_Prediction_Testing_recall',\n",
    "       'Report_After_CatBoost_Prediction_Testing_f1',\n",
    "       'Report_Before_CatBoost_Sensitive_Training_Accuracy',\n",
    "       'Report_Before_CatBoost_Sensitive_Training_precision',\n",
    "       'Report_Before_CatBoost_Sensitive_Training_recall',\n",
    "       'Report_Before_CatBoost_Sensitive_Training_f1',\n",
    "       'Report_Before_CatBoost_Sensitive_Testing_Accuracy',\n",
    "       'Report_Before_CatBoost_Sensitive_Testing_precision',\n",
    "       'Report_Before_CatBoost_Sensitive_Testing_recall',\n",
    "       'Report_Before_CatBoost_Sensitive_Testing_f1',\n",
    "       'Report_After_CatBoost_Sensitive_Training_Accuracy',\n",
    "       'Report_After_CatBoost_Sensitive_Training_precision',\n",
    "       'Report_After_CatBoost_Sensitive_Training_recall',\n",
    "       'Report_After_CatBoost_Sensitive_Training_f1',\n",
    "       'Report_After_CatBoost_Sensitive_Testing_Accuracy',\n",
    "       'Report_After_CatBoost_Sensitive_Testing_precision',\n",
    "       'Report_After_CatBoost_Sensitive_Testing_recall',\n",
    "       'Report_After_CatBoost_Sensitive_Testing_f1',\n",
    "       'Fairness_Equal_Odd_Before_CatBoost_prediction_Training',\n",
    "       'Fairness_Equal_Odd_Before_CatBoost_prediction_Testing',\n",
    "       'Fairness_Equal_Odd_DNN_prediction_Training',\n",
    "       'Fairness_Equal_Odd_DNN_prediction_Testing',\n",
    "       'Fairness_Equal_Odd_After_CatBoost_prediction_Training',\n",
    "       'Fairness_Equal_Odd_After_CatBoost_prediction_Testing',\n",
    "       'Fairness_Equal_Opportunity_Before_CatBoost_prediction_Training',\n",
    "       'Fairness_Equal_Opportunity_Before_CatBoost_prediction_Testing',\n",
    "       'Fairness_Equal_Opportunity_DNN_prediction_Training',\n",
    "       'Fairness_Equal_Opportunity_DNN_prediction_Testing',\n",
    "       'Fairness_Equal_Opportunity_After_CatBoost_prediction_Training',\n",
    "       'Fairness_Equal_Opportunity_After_CatBoost_prediction_Testing',\n",
    "       'Fairness_Demographic_Parity_Before_CatBoost_prediction_Training',\n",
    "       'Fairness_Demographic_Parity_Before_CatBoost_prediction_Testing',\n",
    "       'Fairness_Demographic_Parity_DNN_prediction_Training',\n",
    "       'Fairness_Demographic_Parity_DNN_prediction_Testing',\n",
    "       'Fairness_Demographic_Parity_After_CatBoost_prediction_Training',\n",
    "       'Fairness_Demographic_Parity_After_CatBoost_prediction_Testing',\n",
    "       'Fairness_Treatment_Equality_Before_CatBoost_prediction_Training',\n",
    "       'Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing',\n",
    "       'Fairness_Treatment_Equality_DNN_prediction_Training',\n",
    "       'Fairness_Treatment_Equality_DNN_prediction_Testing',\n",
    "       'Fairness_Treatment_Equality_After_CatBoost_prediction_Training',\n",
    "       'Fairness_Treatment_Equality_After_CatBoost_prediction_Testing'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1RDPCrYfC0f"
   },
   "source": [
    "# **Bootstrap Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-4aiKozcfRFJ"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_date = datetime.now()\n",
    "Data = current_date.strftime(\"%Y-%m-%d %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1641/3273344808.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sampled_df = pd.concat([sampled_df, sampled_group])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_count 1 count: 1 iter: <built-in function iter> Sensitive: 1 Class: 0  a:0.33 b:0.33 r:0.3399999999999999\n",
      "|.."
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import seaborn as sns\n",
    "# Enable anomaly detection\n",
    "def remove_duplicate_columns(df):\n",
    "    df_transposed = df.T\n",
    "    df_unique = df_transposed.drop_duplicates(keep='first').T\n",
    "    return df_unique\n",
    "\n",
    "#=========================Demographic Parity Loss for training purposes=============================\n",
    "def demographic_parity_loss(predictor_pred, sen_batch):\n",
    "    # Calculate the probabilities of positive outcome for each group\n",
    "    positive_outcome_prob_group1 = predictor_pred[sen_batch == 0].mean()\n",
    "    positive_outcome_prob_group2 = predictor_pred[sen_batch == 1].mean()\n",
    "    # The loss is the absolute difference between these probabilities\n",
    "    dp_loss = abs(positive_outcome_prob_group1 - positive_outcome_prob_group2)\n",
    "\n",
    "    return dp_loss\n",
    "\n",
    "#########################################################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def update_hyperparameters(alpha, beta, gamma, loss_gen_to_help_pred, loss_gen_to_fool_dis, dp_loss_predictor, learning_rate, alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Updates the hyperparameters alpha, beta, and gamma using the Adam optimizer based on the losses from the latest model training iteration.\n",
    "\n",
    "    Parameters:\n",
    "    alpha (float): Current value of alpha.\n",
    "    beta (float): Current value of beta.\n",
    "    gamma (float): Current value of gamma.\n",
    "    loss_gen_to_help_pred (float): Latest loss associated with the predictor helping term.\n",
    "    loss_gen_to_fool_dis (float): Latest loss associated with the discriminator fooling term.\n",
    "    dp_loss_predictor (float): Latest loss associated with the fairness loss term for the predictor.\n",
    "    learning_rate (float): Learning rate for the hyperparameter updates.\n",
    "    alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v: Moving averages for Adam optimizer.\n",
    "    t (int): Timestep for Adam optimizer.\n",
    "    beta1, beta2 (float): Parameters for Adam optimizer.\n",
    "    epsilon (float): Small value to prevent division by zero in Adam.\n",
    "\n",
    "    # Initialize Adam parameters\n",
    "    alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t = 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Updated values of (alpha, beta, gamma) and their Adam parameters.\n",
    "    \"\"\"\n",
    "    # Calculate gradients based on inverse loss proportion\n",
    "    total_loss = loss_gen_to_help_pred + loss_gen_to_fool_dis + dp_loss_predictor\n",
    "    grad_alpha = -(loss_gen_to_help_pred / total_loss)\n",
    "    grad_beta = -(dp_loss_predictor / total_loss)\n",
    "    grad_gamma = -(loss_gen_to_fool_dis / total_loss)\n",
    "\n",
    "    # Update hyperparameters using Adam\n",
    "    def adam_update(param, grad, m, v, t):\n",
    "        t += 1\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        param -= learning_rate * m_hat / (np.sqrt(v_hat.detach().numpy()) + epsilon)\n",
    "        return param, m, v, t\n",
    "\n",
    "    alpha, alpha_m, alpha_v, t = adam_update(alpha, grad_alpha, alpha_m, alpha_v, t)\n",
    "    beta, beta_m, beta_v, t = adam_update(beta, grad_beta, beta_m, beta_v, t)\n",
    "    gamma, gamma_m, gamma_v, t = adam_update(gamma, grad_gamma, gamma_m, gamma_v, t)\n",
    "\n",
    "    return (alpha, beta, gamma, alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t)\n",
    "\n",
    "#########################################################################################################################################\n",
    "\n",
    "autograd.set_detect_anomaly(True)\n",
    "Dataset_name = \"German_Credit\"\n",
    "ds_count =0\n",
    "Balance_Sensitive = 1\n",
    "Balance_Class = 0\n",
    "\n",
    "\n",
    "ds_count+=1\n",
    "#------------------Balancing the dataset------------------------------------------------------------\n",
    "balanced_full_dataset = ohe_full_dataset.copy()\n",
    "\n",
    "if Balance_Sensitive==1 and Balance_Class==1:\n",
    "    balanced_full_dataset = equal_sampling(ohe_full_dataset, ['Sensitive','Class'])\n",
    "elif Balance_Sensitive==1 :\n",
    "    balanced_full_dataset = equal_sampling(ohe_full_dataset, ['Sensitive'])\n",
    "elif Balance_Class==1 :\n",
    "    balanced_full_dataset = equal_sampling(ohe_full_dataset, ['Class'])\n",
    "else:\n",
    "    balanced_full_dataset = ohe_full_dataset.loc[:,list(df_X.columns)+[\"Sensitive\",\"Class\"]]\n",
    "\n",
    "balanced_full_dataset = balanced_full_dataset.loc[:,list(df_X.columns)+[\"Sensitive\",\"Class\"]]\n",
    "balanced_full_dataset = balanced_full_dataset.dropna()\n",
    "balanced_full_dataset = remove_duplicate_columns(balanced_full_dataset)\n",
    "Dataset_fairness_Demographic_Parity = fair_Demographic_Parity(balanced_full_dataset,y_var='Class',sensitive_var='Sensitive',prediction='Class')\n",
    "\n",
    "# Selecting specific variables and dropping NA\n",
    "X_var = list(df_X.columns)\n",
    "Sen_var = df_sen.columns\n",
    "y_var = df_y.columns\n",
    "for i in X_var:\n",
    "    if i not in balanced_full_dataset.columns:\n",
    "        X_var.remove(i)\n",
    "\n",
    "# Splitting the dataset\n",
    "# X = balanced_full_dataset.loc[:, list(df_X.columns)].values\n",
    "X = balanced_full_dataset\n",
    "X.drop(['Class',\"Sensitive\"], axis=1)\n",
    "y = balanced_full_dataset.loc[:,  \"Class\"].values\n",
    "sen = balanced_full_dataset.loc[:, [\"Sensitive\"]].values\n",
    "X_var = list(X.columns)\n",
    "#trainspose the dataset\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "XT = scaler.transform(X)\n",
    "random_int = random.randint(0, 10000)\n",
    "# Assuming X, y, and z are your datasets\n",
    "X_train, X_test, y_train, y_test,sen_train, sen_test = train_test_split(XT, y,sen, test_size=0.3, random_state=random_int)\n",
    "X_train, X_test, y_train, y_test,sen_train, sen_test = X_train.astype(np.float32), X_test.astype(np.float32), y_train.astype(np.float32), y_test.astype(np.float32),sen_train.astype(np.float32), sen_test.astype(np.float32)\n",
    "y_train, y_test,sen_train, sen_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1),sen_train.reshape(-1, 1), sen_test.reshape(-1, 1)\n",
    "\n",
    "Dataset_training_size = len(y_train)\n",
    "Dataset_testing_size = len(y_test)\n",
    "\n",
    "count=0\n",
    "Alpha = 0.33\n",
    "Beta = 0.33\n",
    "Gamma = 1-Beta-Alpha\n",
    "time=[]\n",
    "while count<50:\n",
    "    count+=1\n",
    "    print(\"Dataset_count\",ds_count,\"count:\",str(count),\"iter:\",iter,\"Sensitive:\",\n",
    "          Balance_Sensitive,\"Class:\",Balance_Class,' a:'+str(Alpha)+' b:'+str(Beta)+' r:'+str(Gamma))\n",
    "\n",
    "    #=================================================== Model ====================================================\n",
    "    #########################################\n",
    "    # Attention Module (Feature-Attention)\n",
    "    #########################################\n",
    "    class FeatureAttention(nn.Module):\n",
    "        def __init__(self, dim, reduction=4):\n",
    "            super(FeatureAttention, self).__init__()\n",
    "            self.fc1 = nn.Linear(dim, dim // reduction)\n",
    "            self.fc2 = nn.Linear(dim // reduction, dim)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # x shape: (batch_size, dim)\n",
    "            att = self.fc1(x)\n",
    "            att = F.leaky_relu(att, 0.2)\n",
    "            att = self.fc2(att)\n",
    "            att = self.sigmoid(att)\n",
    "            return x * att\n",
    "    \n",
    "    #########################################\n",
    "    # Generator with Increased Depth, Neurons,\n",
    "    # Normalization, Dropout, Residual Connection, and Attention\n",
    "    #########################################\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Generator, self).__init__()\n",
    "            # Expanded layers with increased neurons\n",
    "            self.gen1 = nn.Linear(X_train.shape[1], 64)\n",
    "            self.ln1 = nn.LayerNorm(64)\n",
    "            \n",
    "            self.gen2 = nn.Linear(64, 128)\n",
    "            self.ln2 = nn.LayerNorm(128)\n",
    "            \n",
    "            self.gen3 = nn.Linear(128, 256)\n",
    "            self.ln3 = nn.LayerNorm(256)\n",
    "            \n",
    "            self.gen4 = nn.Linear(256, 256)\n",
    "            self.ln4 = nn.LayerNorm(256)\n",
    "            \n",
    "            self.gen5 = nn.Linear(256, 128)\n",
    "            self.ln5 = nn.LayerNorm(128)\n",
    "            \n",
    "            self.gen6 = nn.Linear(128, 64)\n",
    "            self.ln6 = nn.LayerNorm(64)\n",
    "            \n",
    "            self.gen7 = nn.Linear(64, X_train.shape[1])\n",
    "            \n",
    "            # Attention layer is applied after the last hidden block\n",
    "            self.attn = FeatureAttention(64)\n",
    "            \n",
    "            # Dropout for regularization\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = F.leaky_relu(self.ln1(self.gen1(x)), 0.2)\n",
    "            residual = x  # Save for residual connection (64-dim)\n",
    "            \n",
    "            x = F.leaky_relu(self.ln2(self.gen2(x)), 0.2)\n",
    "            x = self.dropout(x)\n",
    "            x = F.leaky_relu(self.ln3(self.gen3(x)), 0.2)\n",
    "            x = self.dropout(x)\n",
    "            x = F.leaky_relu(self.ln4(self.gen4(x)), 0.2)\n",
    "            x = self.dropout(x)\n",
    "            x = F.leaky_relu(self.ln5(self.gen5(x)), 0.2)\n",
    "            x = self.dropout(x)\n",
    "            x = F.leaky_relu(self.ln6(self.gen6(x)), 0.2)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            # Residual connection: add features from the first layer\n",
    "            x = x + residual\n",
    "            \n",
    "            # Apply attention to recalibrate features\n",
    "            x = self.attn(x)\n",
    "            \n",
    "            return torch.sigmoid(self.gen7(x))\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            # Discriminator\n",
    "            self.dis1 = nn.Linear(X_train.shape[1], 32)\n",
    "            self.dis2 = nn.Linear(32, 64)\n",
    "            self.dis3 = nn.Linear(64, 32)\n",
    "            self.dis4 = nn.Linear(32, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.leaky_relu(self.dis1(x), 0.1)\n",
    "            x = F.leaky_relu(self.dis2(x), 0.1)\n",
    "            x = F.leaky_relu(self.dis3(x), 0.1)\n",
    "            discriminator_output = F.sigmoid(self.dis4(x))\n",
    "            return discriminator_output\n",
    "\n",
    "    class Predictor(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Predictor, self).__init__()\n",
    "            # Predictor\n",
    "            self.pred1 = nn.Linear(X_train.shape[1], 32)\n",
    "            self.pred2 = nn.Linear(32, 64)\n",
    "            self.pred3 = nn.Linear(64, 128)\n",
    "            self.pred4 = nn.Linear(128, 64)\n",
    "            self.pred5 = nn.Linear(64, 32)\n",
    "            self.pred6 = nn.Linear(32, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.leaky_relu(self.pred1(x), 0.3)\n",
    "            x = F.leaky_relu(self.pred2(x), 0.3)\n",
    "            x = F.leaky_relu(self.pred3(x), 0.3)\n",
    "            x = F.leaky_relu(self.pred4(x), 0.3)\n",
    "            x = F.leaky_relu(self.pred5(x), 0.3)\n",
    "            prediction_output = F.sigmoid(self.pred6(x))\n",
    "\n",
    "            return prediction_output\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    #================== Create the training and testing dataloaders Train ==============================\n",
    "\n",
    "    # Converting to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    sen_tensor = torch.tensor(sen_train, dtype=torch.float32)\n",
    "\n",
    "    # Creating a TensorDataset\n",
    "    dataset = TensorDataset(X_tensor, y_tensor, sen_tensor)\n",
    "\n",
    "    # Create a DataLoader for batch processing\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    #================== Create the training and testing dataloaders Test ================================\n",
    "\n",
    "    # Converting to PyTorch tensors\n",
    "    X_Test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_Test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "    sen_Test_tensor = torch.tensor(sen_test, dtype=torch.float32)\n",
    "\n",
    "    # Creating a TensorDataset\n",
    "    dataset_Test = TensorDataset(X_Test_tensor, y_Test_tensor, sen_Test_tensor)\n",
    "\n",
    "    # Create a DataLoader for batch processing\n",
    "    Test_data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "\n",
    "    #=============================================================================================\n",
    "    #               Optimizer Expected values for accuracy, fairness and privacy\n",
    "    #==============================================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "    from catboost import CatBoostClassifier\n",
    "    \n",
    "    Before_catboost_model_class = CatBoostClassifier(border_count= 78, iterations= 30,\n",
    "                                        l2_leaf_reg= 1, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                        loss_function='CrossEntropy',verbose=0)\n",
    "\n",
    "    Before_catboost_model_class.fit(X_tensor.detach().numpy(), y_tensor.numpy())\n",
    "    class_labels = Before_catboost_model_class.predict(X_Test_tensor.detach().numpy())\n",
    "    report = classification_report(y_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Expected_predictor_accuracy =  df_report['precision']['accuracy']\n",
    "    Expected_predictor_fairness = 0.01\n",
    "    Expected_discriminator_accuracy = 0.50\n",
    "\n",
    "    #================================Model, loss function and optimizer Initiation======================\n",
    "    # Instantiate models\n",
    "    generator = Generator()\n",
    "    discriminator = Discriminator()\n",
    "    predictor = Predictor()\n",
    "\n",
    "    # Define loss functions and optimizers\n",
    "    criterion_fake_real = nn.BCELoss()\n",
    "    criterion_classification = nn.BCELoss()\n",
    "    criterion_prediction = nn.BCELoss()\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=0.001)\n",
    "    optimizer_dis = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "    optimizer_pred = optim.Adam(predictor.parameters(), lr=0.002)\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    #                                   Training Loop\n",
    "    ####################################################################################################\n",
    "    num_epochs = 50  # Number of epochs\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    Date = current_date.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch%10==0:\n",
    "            print(\"|\",end=\"\")\n",
    "        else:\n",
    "            print(\".\",end=\"\")\n",
    "        for batch in data_loader:\n",
    "            # Load batch data\n",
    "            X_batch, y_batch, sen_batch = batch\n",
    "\n",
    "            # Reset gradients for all optimizers\n",
    "            optimizer_gen.zero_grad()\n",
    "            optimizer_dis.zero_grad()\n",
    "            optimizer_pred.zero_grad()\n",
    "\n",
    "            # Forward pass through generator\n",
    "            gen_data = generator(X_batch)\n",
    "\n",
    "            # Discriminator Training\n",
    "            # Discriminator tries to predict sensitive attribute from generated data\n",
    "            discriminator_pred = discriminator(gen_data)\n",
    "            loss_discriminator = criterion_fake_real(discriminator_pred, sen_batch)\n",
    "            loss_discriminator.backward(retain_graph=True)  # Retain graph for generator's update\n",
    "            optimizer_dis.step()\n",
    "\n",
    "            # Predictor Training\n",
    "            # Predictor tries to predict target from generated data\n",
    "            predictor_pred = predictor(gen_data)\n",
    "            dp_loss_predictor = demographic_parity_loss(predictor_pred, sen_batch)\n",
    "            loss_predictor = criterion_prediction(predictor_pred, y_batch) + (dp_loss_predictor* Beta)\n",
    "            loss_predictor.backward(retain_graph=True)  # Retain graph for generator's update\n",
    "            optimizer_pred.step()\n",
    "\n",
    "            # Generator Training\n",
    "            discriminator_pred_for_gen = discriminator(gen_data)\n",
    "            loss_gen_to_fool_dis = criterion_fake_real(discriminator_pred_for_gen, 1 - sen_batch)# Generator tries to fool discriminator and help predictor\n",
    "            disc_accuracy = accuracy_score(np.round(discriminator_pred_for_gen.detach().numpy()), sen_batch)# Fooling discriminator\n",
    "\n",
    "            # Helping predictor\n",
    "            predictor_pred_for_gen = predictor(gen_data)\n",
    "            loss_gen_to_help_pred = criterion_classification(predictor_pred_for_gen, y_batch)\n",
    "            pred_accuracy = accuracy_score(np.round(predictor_pred_for_gen.detach().numpy()), y_batch)\n",
    "\n",
    "            # Combine losses for generator\n",
    "            dp_loss_predictor = demographic_parity_loss(predictor(generator(X_batch)), sen_batch)\n",
    "            total_loss_gen =   Alpha*loss_gen_to_help_pred*5 + Gamma*loss_gen_to_fool_dis + Beta*dp_loss_predictor\n",
    "            total_loss_gen.backward()\n",
    "            optimizer_gen.step()\n",
    "\n",
    "\n",
    "    Test_class_pred = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    Test_sen_pred = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    Test_pred_accuracy = accuracy_score(Test_class_pred, y_Test_tensor.numpy())\n",
    "    Test_dp_loss_predictor = demographic_parity_loss(Test_sen_pred, sen_Test_tensor.numpy())\n",
    "    Test_discriminator_pred = np.where(discriminator( generator(X_Test_tensor))>= 0.5, 1, 0)\n",
    "    Test_discriminator_accuracy = accuracy_score(Test_discriminator_pred, sen_Test_tensor.numpy())\n",
    "\n",
    "    Test_discriminator_pred_for_gen = discriminator(generator(X_Test_tensor))\n",
    "    Test_loss_gen_to_fool_dis = criterion_fake_real(Test_discriminator_pred_for_gen, 1 - sen_Test_tensor)\n",
    "    Test_loss_gen_to_help_pred = criterion_classification(predictor( generator(X_Test_tensor)), y_Test_tensor)\n",
    "    Test_dp_loss_predictor = demographic_parity_loss(predictor(generator(X_Test_tensor)), sen_Test_tensor)\n",
    "    Test_total_loss_gen =   Test_loss_gen_to_help_pred + Test_loss_gen_to_fool_dis + Test_dp_loss_predictor\n",
    "\n",
    "    Train_class_pred = np.where(predictor( generator(X_tensor)) >= 0.5, 1, 0)\n",
    "    Train_sen_pred = np.where(predictor( generator(X_tensor)) >= 0.5, 1, 0)\n",
    "    Train_pred_accuracy = accuracy_score(Train_class_pred, y_tensor.numpy())\n",
    "    Train_dp_loss_predictor = demographic_parity_loss(Train_sen_pred, sen_tensor.numpy())\n",
    "    Train_discriminator_pred = np.where(discriminator( generator(X_tensor))>= 0.5, 1, 0)\n",
    "    Train_discriminator_accuracy = accuracy_score(Train_discriminator_pred, sen_tensor.numpy())\n",
    "\n",
    "    Train_discriminator_pred_for_gen = discriminator(generator(X_tensor))\n",
    "    Train_loss_gen_to_fool_dis = criterion_fake_real(Train_discriminator_pred_for_gen, 1 - sen_tensor)\n",
    "    Train_loss_gen_to_help_pred = criterion_classification(predictor( generator(X_tensor)), y_tensor)\n",
    "    Train_dp_loss_predictor = demographic_parity_loss(predictor(generator(X_batch)), sen_batch)\n",
    "    Train_total_loss_gen =   Train_loss_gen_to_help_pred + Train_loss_gen_to_fool_dis + Train_dp_loss_predictor\n",
    "\n",
    "    current_time = datetime.now()\n",
    "    delta = current_time - start_time\n",
    "    time+=[delta]\n",
    "    \n",
    "\n",
    "    #=================================================================\n",
    "    #              Plot the prediction probabilities\n",
    "    #=================================================================\n",
    "    class_labels = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(y_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Predictor_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    #---------------------------------------------------------------------------------------------------------------------------\n",
    "    class_labels = np.where(discriminator( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(sen_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_DNN_Discriminator_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    #---------------------------------------------------------------------------------------------------------------------------\n",
    "    catboost_model = CatBoostClassifier(border_count= 78, depth= 5, iterations= 20,\n",
    "                                                            l2_leaf_reg= 3, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                                            loss_function='CrossEntropy',verbose=0)\n",
    "    catboost_model.fit(generator(X_tensor).detach().numpy(), sen_tensor.numpy())\n",
    "    class_labels = catboost_model.predict(generator(X_Test_tensor).detach().numpy())\n",
    "    report = classification_report(sen_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_After_CatBoost_Sensitive_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    #---------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    probabilities_class = np.array(predictor(generator(X_Test_tensor)).detach())[:,0]\n",
    "    probabilities_sex = np.array(discriminator(generator(X_Test_tensor)).detach())[:,0]\n",
    "    class_labels = catboost_model.predict_proba(generator(X_Test_tensor).detach().numpy())\n",
    "    probabilities_sex_catB = np.array(class_labels[:,1])\n",
    "    \n",
    "    # Define the bin width\n",
    "    bin_width = 0.02\n",
    "    \n",
    "    # Determine the global minimum and maximum to ensure all histograms use the same bins\n",
    "    min_prob = 0\n",
    "    max_prob = 1\n",
    "    \n",
    "    # Create bins from the minimum to the maximum value with the specified bin width\n",
    "    bins = np.arange(min_prob, max_prob + bin_width, bin_width)\n",
    "    \n",
    "    # Format accuracy values to three decimal points\n",
    "    formatted_class_accuracy = f\"{Report_Predictor_Testing_Accuracy:.3f}\"\n",
    "    formatted_gender_accuracy = f\"{Report_DNN_Discriminator_Testing_Accuracy:.3f}\"\n",
    "    formatted_gender_catb_accuracy = f\"{Report_After_CatBoost_Sensitive_Testing_Accuracy:.3f}\"\n",
    "    \n",
    "    # Plot histograms with formatted labels\n",
    "    plt.hist(probabilities_class, bins=bins, density=True, alpha=0.5, label=f'Class (Acc: {formatted_class_accuracy})')\n",
    "    plt.hist(probabilities_sex, bins=bins, density=True, alpha=0.5, label=f'Gender (Acc: {formatted_gender_accuracy})')\n",
    "    plt.hist(probabilities_sex_catB, bins=bins, density=True, alpha=0.5, label=f'Gender_CatB (Acc: {formatted_gender_catb_accuracy})')\n",
    "    \n",
    "    plt.legend(title='Probability Distributions')\n",
    "    plt.title('Density Plot of Prediction Probabilities at CM: '+' a:'+str(Alpha)+' b:'+str(Beta)+' r:'+str(Gamma))\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Density')\n",
    "    plt.yscale('log')  # Set the y-axis to logarithmic scale\n",
    "    plt.savefig('Plots_op_2/Density Plot of Prediction Probabilities at CM: ' + ' a:'+str(np.round(Alpha,2))+' b:'+str(np.round(Beta,2))+' r:'+str(np.round(Gamma,2))+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    from sklearn.metrics import log_loss\n",
    "    from catboost import CatBoostClassifier\n",
    "    catboost_model = CatBoostClassifier(border_count= 78, depth= 5, iterations= 20,\n",
    "                                        l2_leaf_reg= 3, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                        loss_function='CrossEntropy',verbose=0)\n",
    "\n",
    "    catboost_model.fit(generator(X_tensor).detach().numpy(), sen_tensor.numpy())\n",
    "    class_labels = catboost_model.predict(generator(X_tensor).detach().numpy())\n",
    "    class_labels = 1 - class_labels\n",
    "    cat_sen_bce_loss = log_loss(sen_tensor.numpy(), class_labels)\n",
    "    print('------------cat_dis_loss',cat_sen_bce_loss)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #=================================================================\n",
    "    #               Discriminator report Training\n",
    "    #=================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = np.where(discriminator( generator(X_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(sen_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_DNN_Discriminator_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_DNN_Discriminator_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_DNN_Discriminator_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_DNN_Discriminator_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Discriminator report Tessting\n",
    "    #=================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = np.where(discriminator( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(sen_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_DNN_Discriminator_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_DNN_Discriminator_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_DNN_Discriminator_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_DNN_Discriminator_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Predictor report Training\n",
    "    #=================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = np.where(predictor( generator(X_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(y_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Predictor_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Predictor_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_Predictor_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_Predictor_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Predictor report Testing\n",
    "    #=================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(y_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Predictor_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Predictor_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_Predictor_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_Predictor_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "\n",
    "    #=================================================================\n",
    "    #               Before CatBoost Predictor report - Training\n",
    "    #=================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    from catboost import CatBoostClassifier\n",
    "    Before_catboost_model_class = CatBoostClassifier(border_count= 78, depth= 5, iterations= 20,\n",
    "                                        l2_leaf_reg= 3, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                        loss_function='CrossEntropy',verbose=0)\n",
    "\n",
    "    Before_catboost_model_class.fit(X_tensor.detach().numpy(), y_tensor.numpy())\n",
    "    print(\"Training Score:\",Before_catboost_model_class.score(X_tensor.detach().numpy(), y_tensor.numpy()))\n",
    "\n",
    "    class_labels = Before_catboost_model_class.predict(X_tensor.detach().numpy())\n",
    "    report = classification_report(y_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Before_CatBoost_Prediction_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Before_CatBoost_Prediction_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_Before_CatBoost_Prediction_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_Before_CatBoost_Prediction_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Before CatBoost Predictor report - Testing\n",
    "    #=================================================================\n",
    "    class_labels = Before_catboost_model_class.predict(X_Test_tensor.detach().numpy())\n",
    "    report = classification_report(y_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Before_CatBoost_Prediction_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Before_CatBoost_Prediction_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_Before_CatBoost_Prediction_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_Before_CatBoost_Prediction_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               After CatBoost Predictor report\n",
    "    #=================================================================\n",
    "    from catboost import CatBoostClassifier\n",
    "    After_catboost_model_class = CatBoostClassifier(border_count= 178, depth= 8, iterations= 20,\n",
    "                                        l2_leaf_reg= 1, learning_rate= 0.3, min_data_in_leaf= 5,\n",
    "                                        loss_function='CrossEntropy',verbose=0 )\n",
    "\n",
    "    After_catboost_model_class.fit(generator(X_tensor).detach().numpy(), y_tensor.numpy())\n",
    "    print(\"Training Score:\",After_catboost_model_class.score(generator(X_tensor).detach().numpy(), y_tensor.numpy()))\n",
    "\n",
    "    class_labels = After_catboost_model_class.predict(generator(X_tensor).detach().numpy())\n",
    "    report = classification_report(y_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_After_CatBoost_Prediction_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_After_CatBoost_Prediction_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_After_CatBoost_Prediction_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_After_CatBoost_Prediction_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Before CatBoost Predictor report - Testing\n",
    "    #=================================================================\n",
    "    class_labels = After_catboost_model_class.predict(generator(X_Test_tensor).detach().numpy())\n",
    "    report = classification_report(y_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_After_CatBoost_Prediction_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_After_CatBoost_Prediction_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_After_CatBoost_Prediction_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_After_CatBoost_Prediction_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #            Before CatBoost Sen_Var Predictor report - Training\n",
    "    #=================================================================\n",
    "    from catboost import CatBoostClassifier\n",
    "    catboost_model = CatBoostClassifier(border_count= 78, depth= 5, iterations= 20,\n",
    "                                        l2_leaf_reg= 3, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                        loss_function='CrossEntropy',verbose=0)\n",
    "\n",
    "    catboost_model.fit(X_tensor.detach().numpy(), sen_tensor.numpy())\n",
    "    print(\"Training Score:\",catboost_model.score(X_tensor.detach().numpy(), sen_tensor.numpy()))\n",
    "\n",
    "    class_labels = catboost_model.predict(X_tensor.detach().numpy())\n",
    "    report = classification_report(sen_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Before_CatBoost_Sensitive_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Before_CatBoost_Sensitive_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_Before_CatBoost_Sensitive_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_Before_CatBoost_Sensitive_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Before CatBoost Sen_Var Predictor report - Testing\n",
    "    #=================================================================\n",
    "    class_labels = catboost_model.predict(X_Test_tensor.detach().numpy())\n",
    "    report = classification_report(sen_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Before_CatBoost_Sensitive_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Before_CatBoost_Sensitive_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_Before_CatBoost_Sensitive_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_Before_CatBoost_Sensitive_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #            After CatBoost Sen_Var Predictor report - Training\n",
    "    #=================================================================\n",
    "    from catboost import CatBoostClassifier\n",
    "    catboost_model = CatBoostClassifier(border_count= 78, depth= 5, iterations= 20,\n",
    "                                        l2_leaf_reg= 3, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                        loss_function='CrossEntropy',verbose=0)\n",
    "\n",
    "    catboost_model.fit(generator(X_tensor).detach().numpy(), sen_tensor.numpy())\n",
    "    print(\"Training Score:\",catboost_model.score(generator(X_tensor).detach().numpy(), sen_tensor.numpy()))\n",
    "\n",
    "    class_labels = catboost_model.predict(generator(X_tensor).detach().numpy())\n",
    "    report = classification_report(sen_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_After_CatBoost_Sensitive_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_After_CatBoost_Sensitive_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_After_CatBoost_Sensitive_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_After_CatBoost_Sensitive_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               After CatBoost Sen_Var Predictor report - Testing\n",
    "    #=================================================================\n",
    "    class_labels = catboost_model.predict(generator(X_Test_tensor).detach().numpy())\n",
    "    report = classification_report(sen_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_After_CatBoost_Sensitive_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_After_CatBoost_Sensitive_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_After_CatBoost_Sensitive_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_After_CatBoost_Sensitive_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "\n",
    "    #=================================================================\n",
    "    #               Merge the data to a single data-frame\n",
    "    #=================================================================\n",
    "    Training_df = pd.DataFrame(X_train,columns=X_var)\n",
    "    Training_df[\"sen\"] = sen_train\n",
    "    Training_df[\"pred\"] = y_train\n",
    "\n",
    "    Training_df[\"After_prediction_DNN\"] = np.where(predictor( generator(X_tensor)) >= 0.5, 1, 0)\n",
    "    Training_df[\"After_prediction_CatB\"] = After_catboost_model_class.predict(generator(X_tensor).detach().numpy())\n",
    "    Training_df[\"Before_prediction_CatB\"] = Before_catboost_model_class.predict(X_tensor.detach().numpy())\n",
    "\n",
    "\n",
    "    Testing_df = pd.DataFrame(X_test,columns=X_var)\n",
    "    Testing_df[\"sen\"] = sen_test\n",
    "    Testing_df[\"pred\"] = y_test\n",
    "\n",
    "    Testing_df[\"After_prediction_DNN\"] = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    Testing_df[\"After_prediction_CatB\"] = After_catboost_model_class.predict(generator(X_Test_tensor).detach().numpy())\n",
    "    Testing_df[\"Before_prediction_CatB\"] = Before_catboost_model_class.predict(X_Test_tensor.detach().numpy())\n",
    "\n",
    "    #=================================================================\n",
    "    #              Plot the prediction probabilities\n",
    "    #=================================================================\n",
    "    probabilities_class = np.array(predictor(generator(X_Test_tensor)).detach())[:,0]\n",
    "    probabilities_sex = np.array(discriminator(generator(X_Test_tensor)).detach())[:,0]\n",
    "    class_labels = catboost_model.predict_proba(generator(X_Test_tensor).detach().numpy())\n",
    "    probabilities_sex_catB = np.array(class_labels[:,1])\n",
    "    \n",
    "    # Define the bin width\n",
    "    bin_width = 0.02\n",
    "    \n",
    "    # Determine the global minimum and maximum to ensure all histograms use the same bins\n",
    "    min_prob = min(probabilities_class.min(), probabilities_sex.min(), probabilities_sex_catB.min())\n",
    "    max_prob = max(probabilities_class.max(), probabilities_sex.max(), probabilities_sex_catB.max())\n",
    "    \n",
    "    # Create bins from the minimum to the maximum value with the specified bin width\n",
    "    bins = np.arange(min_prob, max_prob + bin_width, bin_width)\n",
    "    \n",
    "    # Format accuracy values to three decimal points\n",
    "    formatted_class_accuracy = f\"{Report_Predictor_Testing_Accuracy:.3f}\"\n",
    "    formatted_gender_accuracy = f\"{Report_DNN_Discriminator_Testing_Accuracy:.3f}\"\n",
    "    formatted_gender_catb_accuracy = f\"{Report_After_CatBoost_Sensitive_Testing_Accuracy:.3f}\"\n",
    "    \n",
    "    # Plot histograms with formatted labels\n",
    "    plt.hist(probabilities_class, bins=bins, density=True, alpha=0.5, label=f'Class (Acc: {formatted_class_accuracy})')\n",
    "    plt.hist(probabilities_sex, bins=bins, density=True, alpha=0.5, label=f'Gender (Acc: {formatted_gender_accuracy})')\n",
    "    plt.hist(probabilities_sex_catB, bins=bins, density=True, alpha=0.5, label=f'Gender_CatB (Acc: {formatted_gender_catb_accuracy})')\n",
    "    \n",
    "    plt.legend(title='Probability Distributions')\n",
    "    plt.title('Density Plot of Prediction Probabilities at CM: ' + ' a:'+str(Alpha)+' b:'+str(Beta)+' r:'+str(Gamma))\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Density')\n",
    "    # plt.yscale('log')  # Set the y-axis to logarithmic scale\n",
    "    plt.savefig('plots_op_4_GC/Density Plot of Prediction Probabilities at CM: ' + ' a:'+str(np.round(Alpha,2))+' b:'+str(np.round(Beta,2))+' r:'+str(np.round(Gamma,2))+'.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #=================================================================\n",
    "    #                  Equal Odd\n",
    "    #=================================================================\n",
    "    Fairness_Equal_Odd_Before_CatBoost_prediction_Training = fair_Eq_Odds(Training_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Equal_Odd_Before_CatBoost_prediction_Testing = fair_Eq_Odds(Testing_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Equal_Odd_DNN_prediction_Training = fair_Eq_Odds(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Equal_Odd_DNN_prediction_Testing = fair_Eq_Odds(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Equal_Odd_After_CatBoost_prediction_Training = fair_Eq_Odds(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Equal_Odd_After_CatBoost_prediction_Testing = fair_Eq_Odds(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "\n",
    "    #=================================================================\n",
    "    #                  Equal Opportunity\n",
    "    #=================================================================\n",
    "    Fairness_Equal_Opportunity_Before_CatBoost_prediction_Training = fair_Eq_Opportunity(Training_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Equal_Opportunity_Before_CatBoost_prediction_Testing = fair_Eq_Opportunity(Testing_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Equal_Opportunity_DNN_prediction_Training = fair_Eq_Opportunity(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Equal_Opportunity_DNN_prediction_Testing = fair_Eq_Opportunity(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Equal_Opportunity_After_CatBoost_prediction_Training = fair_Eq_Opportunity(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Equal_Opportunity_After_CatBoost_prediction_Testing = fair_Eq_Opportunity(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "\n",
    "    #=================================================================\n",
    "    #                  Demographic Parity\n",
    "    #=================================================================\n",
    "    Fairness_Demographic_Parity_Before_CatBoost_prediction_Training = fair_Demographic_Parity(Training_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Demographic_Parity_Before_CatBoost_prediction_Testing = fair_Demographic_Parity(Testing_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Demographic_Parity_DNN_prediction_Training = fair_Demographic_Parity(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Demographic_Parity_DNN_prediction_Testing = fair_Demographic_Parity(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Demographic_Parity_After_CatBoost_prediction_Training = fair_Demographic_Parity(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Demographic_Parity_After_CatBoost_prediction_Testing = fair_Demographic_Parity(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "\n",
    "    #=================================================================\n",
    "    #                  Treatment Equality\n",
    "    #=================================================================\n",
    "    Fairness_Treatment_Equality_Before_CatBoost_prediction_Training = fair_Treatment_Equality(Training_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing = fair_Treatment_Equality(Testing_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Treatment_Equality_DNN_prediction_Training = fair_Treatment_Equality(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Treatment_Equality_DNN_prediction_Testing = fair_Treatment_Equality(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Treatment_Equality_After_CatBoost_prediction_Training = fair_Treatment_Equality(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Treatment_Equality_After_CatBoost_prediction_Testing = fair_Treatment_Equality(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "\n",
    "    #=================================================================\n",
    "    #                  create the results table\n",
    "    #=================================================================\n",
    "\n",
    "    current_date = datetime.now()\n",
    "    Date = current_date.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "\n",
    "    results = { \"Data\": [Date],\n",
    "    \"Dataset_name\": [Dataset_name],\n",
    "    \"Alpha-pred\": [Alpha],\n",
    "    \"Beta-Fair\": [Beta],\n",
    "    \"Gamma-privacy\": [Gamma],\n",
    "    # \"cost_multiplier-all\": [cost_multiplier],\n",
    "    \"Balance_Class\": [Balance_Class] ,\n",
    "    \"Balance_Sensitive\": [Balance_Sensitive],\n",
    "    \"Dataset_training_size\": [Dataset_training_size],\n",
    "    \"Dataset_testing_size\": [Dataset_testing_size],\n",
    "    # \"Fairness_cost_function\": [Fairness_cost_function],\n",
    "    # \"Fairness_cost_multiplier\": [Fairness_cost_multiplier],\n",
    "    \"Test_loss_predictor\":[Test_loss_gen_to_help_pred.detach().numpy()],\n",
    "    \"Test_loss_Discriminator_fool\": [Test_loss_gen_to_fool_dis.detach().numpy()],\n",
    "    \"Test_total_loss_gen\": [Test_total_loss_gen.detach().numpy()],\n",
    "    \"Train_loss_predictor\":[Train_loss_gen_to_help_pred.detach().numpy()],\n",
    "    \"Train_loss_Discriminator_fool\": [Train_loss_gen_to_fool_dis.detach().numpy()],\n",
    "    \"Train_total_loss_gen\": [Train_total_loss_gen.detach().numpy()],\n",
    "    \"Dataset_fairness_Demographic_Parity\": [Dataset_fairness_Demographic_Parity],\n",
    "    \"Report_DNN_Discriminator_Training_Accuracy\": [Report_DNN_Discriminator_Training_Accuracy],\n",
    "    \"Report_DNN_Discriminator_Training_precision\": [Report_DNN_Discriminator_Training_precision],\n",
    "    \"Report_DNN_Discriminator_Training_recall\": [Report_DNN_Discriminator_Training_recall],\n",
    "    \"Report_DNN_Discriminator_Training_f1\": [Report_DNN_Discriminator_Training_f1],\n",
    "    \"Report_DNN_Discriminator_Testing_Accuracy\": [Report_DNN_Discriminator_Testing_Accuracy],\n",
    "    \"Report_DNN_Discriminator_Testing_precision\": [Report_DNN_Discriminator_Testing_precision],\n",
    "    \"Report_DNN_Discriminator_Testing_recall\": [Report_DNN_Discriminator_Testing_recall],\n",
    "    \"Report_DNN_Discriminator_Testing_f1\": [Report_DNN_Discriminator_Testing_f1],\n",
    "    \"Report_Predictor_Training_Accuracy\": [Report_Predictor_Training_Accuracy],\n",
    "    \"Report_Predictor_Training_precision\": [Report_Predictor_Training_precision],\n",
    "    \"Report_Predictor_Training_recall\": [Report_Predictor_Training_recall],\n",
    "    \"Report_Predictor_Training_f1\": [Report_Predictor_Training_f1],\n",
    "    \"Report_Predictor_Testing_Accuracy\": [Report_Predictor_Testing_Accuracy],\n",
    "    \"Report_Predictor_Testing_precision\": [Report_Predictor_Testing_precision],\n",
    "    \"Report_Predictor_Testing_recall\": [Report_Predictor_Testing_recall],\n",
    "    \"Report_Predictor_Testing_f1\": [Report_Predictor_Testing_f1],\n",
    "    \"Report_Before_CatBoost_Prediction_Training_Accuracy\": [Report_Before_CatBoost_Prediction_Training_Accuracy],\n",
    "    \"Report_Before_CatBoost_Prediction_Training_precision\": [Report_Before_CatBoost_Prediction_Training_precision],\n",
    "    \"Report_Before_CatBoost_Prediction_Training_recall\": [Report_Before_CatBoost_Prediction_Training_recall],\n",
    "    \"Report_Before_CatBoost_Prediction_Training_f1\": [Report_Before_CatBoost_Prediction_Training_f1],\n",
    "    \"Report_Before_CatBoost_Prediction_Testing_Accuracy\": [Report_Before_CatBoost_Prediction_Testing_Accuracy],\n",
    "    \"Report_Before_CatBoost_Prediction_Testing_precision\": [Report_Before_CatBoost_Prediction_Testing_precision],\n",
    "    \"Report_Before_CatBoost_Prediction_Testing_recall\": [Report_Before_CatBoost_Prediction_Testing_recall],\n",
    "    \"Report_Before_CatBoost_Prediction_Testing_f1\": [Report_Before_CatBoost_Prediction_Testing_f1],\n",
    "    \"Report_After_CatBoost_Prediction_Training_Accuracy\": [Report_After_CatBoost_Prediction_Training_Accuracy],\n",
    "    \"Report_After_CatBoost_Prediction_Training_precision\": [Report_After_CatBoost_Prediction_Training_precision],\n",
    "    \"Report_After_CatBoost_Prediction_Training_recall\": [Report_After_CatBoost_Prediction_Training_recall],\n",
    "    \"Report_After_CatBoost_Prediction_Training_f1\": [Report_After_CatBoost_Prediction_Training_f1],\n",
    "    \"Report_After_CatBoost_Prediction_Testing_Accuracy\": [Report_After_CatBoost_Prediction_Testing_Accuracy],\n",
    "    \"Report_After_CatBoost_Prediction_Testing_precision\": [Report_After_CatBoost_Prediction_Testing_precision],\n",
    "    \"Report_After_CatBoost_Prediction_Testing_recall\": [Report_After_CatBoost_Prediction_Testing_recall],\n",
    "    \"Report_After_CatBoost_Prediction_Testing_f1\": [Report_After_CatBoost_Prediction_Testing_f1],\n",
    "    \"Report_Before_CatBoost_Sensitive_Training_Accuracy\": [Report_Before_CatBoost_Sensitive_Training_Accuracy],\n",
    "    \"Report_Before_CatBoost_Sensitive_Training_precision\": [Report_Before_CatBoost_Sensitive_Training_precision],\n",
    "    \"Report_Before_CatBoost_Sensitive_Training_recall\": [Report_Before_CatBoost_Sensitive_Training_recall],\n",
    "    \"Report_Before_CatBoost_Sensitive_Training_f1\": [Report_Before_CatBoost_Sensitive_Training_f1],\n",
    "    \"Report_Before_CatBoost_Sensitive_Testing_Accuracy\": [Report_Before_CatBoost_Sensitive_Testing_Accuracy],\n",
    "    \"Report_Before_CatBoost_Sensitive_Testing_precision\": [Report_Before_CatBoost_Sensitive_Testing_precision],\n",
    "    \"Report_Before_CatBoost_Sensitive_Testing_recall\": [Report_Before_CatBoost_Sensitive_Testing_recall],\n",
    "    \"Report_Before_CatBoost_Sensitive_Testing_f1\": [Report_Before_CatBoost_Sensitive_Testing_f1],\n",
    "    \"Report_After_CatBoost_Sensitive_Training_Accuracy\": [Report_After_CatBoost_Sensitive_Training_Accuracy],\n",
    "    \"Report_After_CatBoost_Sensitive_Training_precision\":[ Report_After_CatBoost_Sensitive_Training_precision],\n",
    "    \"Report_After_CatBoost_Sensitive_Training_recall\": [Report_After_CatBoost_Sensitive_Training_recall],\n",
    "    \"Report_After_CatBoost_Sensitive_Training_f1\": [Report_After_CatBoost_Sensitive_Training_f1],\n",
    "    \"Report_After_CatBoost_Sensitive_Testing_Accuracy\": [Report_After_CatBoost_Sensitive_Testing_Accuracy],\n",
    "    \"Report_After_CatBoost_Sensitive_Testing_precision\": [Report_After_CatBoost_Sensitive_Testing_precision],\n",
    "    \"Report_After_CatBoost_Sensitive_Testing_recall\": [Report_After_CatBoost_Sensitive_Testing_recall],\n",
    "    \"Report_After_CatBoost_Sensitive_Testing_f1\": [Report_After_CatBoost_Sensitive_Testing_f1],\n",
    "    \"Fairness_Equal_Odd_Before_CatBoost_prediction_Training\": [Fairness_Equal_Odd_Before_CatBoost_prediction_Training],\n",
    "    \"Fairness_Equal_Odd_Before_CatBoost_prediction_Testing\": [Fairness_Equal_Odd_Before_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Equal_Odd_DNN_prediction_Training\": [Fairness_Equal_Odd_DNN_prediction_Training],\n",
    "    \"Fairness_Equal_Odd_DNN_prediction_Testing\": [Fairness_Equal_Odd_DNN_prediction_Testing],\n",
    "    \"Fairness_Equal_Odd_After_CatBoost_prediction_Training\": [Fairness_Equal_Odd_After_CatBoost_prediction_Training],\n",
    "    \"Fairness_Equal_Odd_After_CatBoost_prediction_Testing\": [Fairness_Equal_Odd_After_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Equal_Opportunity_Before_CatBoost_prediction_Training\": [Fairness_Equal_Opportunity_Before_CatBoost_prediction_Training],\n",
    "    \"Fairness_Equal_Opportunity_Before_CatBoost_prediction_Testing\": [Fairness_Equal_Opportunity_Before_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Equal_Opportunity_DNN_prediction_Training\": [Fairness_Equal_Opportunity_DNN_prediction_Training],\n",
    "    \"Fairness_Equal_Opportunity_DNN_prediction_Testing\": [Fairness_Equal_Opportunity_DNN_prediction_Testing],\n",
    "    \"Fairness_Equal_Opportunity_After_CatBoost_prediction_Training\": [Fairness_Equal_Opportunity_After_CatBoost_prediction_Training],\n",
    "    \"Fairness_Equal_Opportunity_After_CatBoost_prediction_Testing\": [Fairness_Equal_Opportunity_After_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Demographic_Parity_Before_CatBoost_prediction_Training\": [Fairness_Demographic_Parity_Before_CatBoost_prediction_Training],\n",
    "    \"Fairness_Demographic_Parity_Before_CatBoost_prediction_Testing\": [Fairness_Demographic_Parity_Before_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Demographic_Parity_DNN_prediction_Training\": [Fairness_Demographic_Parity_DNN_prediction_Training],\n",
    "    \"Fairness_Demographic_Parity_DNN_prediction_Testing\": [Fairness_Demographic_Parity_DNN_prediction_Testing],\n",
    "    \"Fairness_Demographic_Parity_After_CatBoost_prediction_Training\": [Fairness_Demographic_Parity_After_CatBoost_prediction_Training],\n",
    "    \"Fairness_Demographic_Parity_After_CatBoost_prediction_Testing\": [Fairness_Demographic_Parity_After_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Treatment_Equality_Before_CatBoost_prediction_Training\": [Fairness_Treatment_Equality_Before_CatBoost_prediction_Training],\n",
    "    \"Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing\": [Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Treatment_Equality_DNN_prediction_Training\": [Fairness_Treatment_Equality_DNN_prediction_Training],\n",
    "    \"Fairness_Treatment_Equality_DNN_prediction_Testing\": [Fairness_Treatment_Equality_DNN_prediction_Testing],\n",
    "    \"Fairness_Treatment_Equality_After_CatBoost_prediction_Training\": [Fairness_Treatment_Equality_After_CatBoost_prediction_Training],\n",
    "    \"Fairness_Treatment_Equality_After_CatBoost_prediction_Testing\": [Fairness_Treatment_Equality_After_CatBoost_prediction_Testing],\n",
    "        }\n",
    "\n",
    "    df_temp_results = pd.DataFrame(results)\n",
    "    df_results = pd.concat([df_results, df_temp_results], ignore_index=True)\n",
    "    df_results.to_csv(Dataset_name+\"_\"+Date[:10]+\"_abl_a0.csv\")\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    #                                  Update Alpha, Beta, Gamma \n",
    "    ####################################################################################################\n",
    "    # Call the function after a training iteration\n",
    "    learning_rate = 1.5\n",
    "    # Assuming initial values for Alpha, Beta, Gamma and their respective Adam optimizer parameters\n",
    "    alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t = 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # When calling the update_hyperparameters function\n",
    "    Alpha, Beta, Gamma, alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t = update_hyperparameters(\n",
    "        Alpha, Beta, Gamma, loss_gen_to_help_pred*100, cat_sen_bce_loss, dp_loss_predictor, learning_rate,\n",
    "        alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t)\n",
    "\n",
    "    Alpha, Beta, Gamma = float(Alpha.detach().numpy()),   float(Beta.detach().numpy()),    float(Gamma.detach().numpy()) \n",
    "    print(f\" {count} Updated Alpha: {Alpha}, Beta: {Beta}, Gamma: {Gamma}\")\n",
    "\n",
    "    class_pred = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    sen_pred = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    pred_accuracy = accuracy_score(class_pred, y_Test_tensor.numpy())\n",
    "    dp_loss_predictor = demographic_parity_loss(sen_pred, sen_Test_tensor.numpy())\n",
    "\n",
    "    # Alpha, Beta, Gamma = float(Alpha.detach().numpy(), Beta.detach().numpy(), Gamma.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
