{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRgVkCt5xaan",
    "outputId": "67d86d41-def8-4860-db76-87e05121c5a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: catboost in /opt/conda/lib/python3.11/site-packages (1.2.7)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.11/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from catboost) (3.9.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.11/site-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.11/site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from catboost) (1.14.1)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.11/site-packages (from catboost) (6.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->catboost) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.11/site-packages (from plotly->catboost) (1.34.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: shap in /opt/conda/lib/python3.11/site-packages (0.47.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from shap) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from shap) (1.6.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /opt/conda/lib/python3.11/site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in /opt/conda/lib/python3.11/site-packages (from shap) (24.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in /opt/conda/lib/python3.11/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /opt/conda/lib/python3.11/site-packages (from shap) (0.61.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.11/site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from shap) (4.13.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: plotly in /opt/conda/lib/python3.11/site-packages (6.0.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.11/site-packages (from plotly) (1.34.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from plotly) (24.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gdown in /opt/conda/lib/python3.11/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from gdown) (4.13.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: PrettyTable in /opt/conda/lib/python3.11/site-packages (3.16.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from PrettyTable) (0.2.13)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gdown in /opt/conda/lib/python3.11/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from gdown) (4.13.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: aif360 in /opt/conda/lib/python3.11/site-packages (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.11/site-packages (from aif360) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from aif360) (1.14.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from aif360) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /opt/conda/lib/python3.11/site-packages (from aif360) (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from aif360) (3.9.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.24.0->aif360) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.24.0->aif360) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.24.0->aif360) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.0->aif360) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.0->aif360) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->aif360) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->aif360) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->aif360) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->aif360) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->aif360) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib->aif360) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->aif360) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.0->aif360) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: category_encoders in /opt/conda/lib/python3.11/site-packages (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from category_encoders) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/lib/python3.11/site-packages (from category_encoders) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from category_encoders) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from category_encoders) (1.14.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from category_encoders) (0.14.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.5->category_encoders) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.6.0->category_encoders) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.11/site-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install catboost\n",
    "!pip install shap\n",
    "!pip install plotly\n",
    "!pip install gdown\n",
    "!pip install PrettyTable\n",
    "!pip install seaborn\n",
    "!pip install gdown\n",
    "!pip install aif360\n",
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cy0HIlItxeGt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gdown\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkRY5XY0225t"
   },
   "source": [
    "# **Fairness Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dMHfad_Hxgro"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from prettytable import PrettyTable\n",
    "from itertools import combinations\n",
    "\n",
    "def fair_Demographic_Parity(X,y_var,sensitive_var,prediction):\n",
    "#   print(\"----------------------------------------------------\")\n",
    "#   print(\"\\t\\t\\t\",sensitive_var)\n",
    "#   print('----------------------------------------------------')\n",
    "  groups = X[sensitive_var].unique()\n",
    "  positive_proportions = []\n",
    "\n",
    "  # Create a PrettyTable object\n",
    "  table = PrettyTable()\n",
    "  table.field_names = [\"Name\", \"proportion_positive\"]\n",
    "\n",
    "  for group in groups:\n",
    "    group_data = X[X[sensitive_var] == group]\n",
    "\n",
    "    y_pred_group = group_data[prediction]\n",
    "    # Calculate Proportion Positive for the current group\n",
    "    proportion_positive = y_pred_group.mean()\n",
    "\n",
    "    # Append Proportion Positive for the current group\n",
    "    positive_proportions.append(proportion_positive)\n",
    "    table.add_row([group, proportion_positive])\n",
    "    # Find the maximum absolute difference in Proportion Positive across groups\n",
    "  max_diff = max(abs(pp1 - pp2) for pp1, pp2 in combinations(positive_proportions, 2))\n",
    "\n",
    "#   print(\"Demographic Parity Score :\",max_diff)\n",
    "#   print(table)\n",
    "  return max_diff\n",
    "\n",
    "def fair_Eq_Odds(X,y_var,sensitive_var,prediction):\n",
    "#   print(\"----------------------------------------------------\")\n",
    "#   print(\"\\t\\t\\t\",sensitive_var)\n",
    "#   print('----------------------------------------------------')\n",
    "  groups = X[sensitive_var].unique()\n",
    "  equal_odds_scores = []\n",
    "\n",
    "  # Create a PrettyTable object\n",
    "  table = PrettyTable()\n",
    "  table.field_names = [\"Name\", \"TPR\", \"FPR\"]\n",
    "\n",
    "  for group in groups:\n",
    "    group_data = X[X[sensitive_var] == group]\n",
    "    y_true_group = group_data[y_var]\n",
    "    y_pred_group = group_data[prediction]\n",
    "\n",
    "    cm = confusion_matrix(y_true_group, y_pred_group)\n",
    "\n",
    "    # Calculate Sensitivity (True Positive Rate)\n",
    "    tpr = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # Calculate Specificity (True Negative Rate)\n",
    "    fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    # Calculate Equal Odds for the current group\n",
    "    equal_odds_scores.append((tpr, fpr))\n",
    "    # Add data to the table\n",
    "    table.add_row([group, tpr, fpr])\n",
    "\n",
    "  # Find the maximum absolute difference in TPR and FPR across groups\n",
    "  max_diff = max(abs(tpr1 - tpr2) + abs(fpr1 - fpr2) for (tpr1, fpr1),\n",
    "   (tpr2, fpr2) in zip(equal_odds_scores[::2], equal_odds_scores[1::2]))\n",
    "\n",
    "#   print(\"Equalized Odds Score :\",max_diff)\n",
    "#   print(table)\n",
    "  return max_diff\n",
    "\n",
    "\n",
    "\n",
    "def fair_Eq_Opportunity(X,y_var,sensitive_var,prediction):\n",
    "#   print(\"----------------------------------------------------\")\n",
    "#   print(\"\\t\\t\\t\",sensitive_var)\n",
    "#   print('----------------------------------------------------')\n",
    "  groups = X[sensitive_var].unique()\n",
    "  equal_odds_scores = []\n",
    "\n",
    "  # Create a PrettyTable object\n",
    "  table = PrettyTable()\n",
    "  table.field_names = [\"Name\", \"TPR\"]\n",
    "\n",
    "  for group in groups:\n",
    "    group_data = X[X[sensitive_var] == group]\n",
    "    y_true_group = group_data[y_var]\n",
    "    y_pred_group = group_data[prediction]\n",
    "\n",
    "    cm = confusion_matrix(y_true_group, y_pred_group)\n",
    "\n",
    "    # Calculate Sensitivity (True Positive Rate)\n",
    "    tpr = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # Calculate Specificity (True Negative Rate)\n",
    "    fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    # Calculate Equal Odds for the current group\n",
    "    equal_odds_scores.append((tpr, fpr))\n",
    "    # Add data to the table\n",
    "    table.add_row([group, tpr])\n",
    "\n",
    "  # Find the maximum absolute difference in TPR and FPR across groups\n",
    "  max_diff = max(abs(tpr1 - tpr2)  for (tpr1, fpr1),\n",
    "   (tpr2, fpr2) in zip(equal_odds_scores[::2], equal_odds_scores[1::2]))\n",
    "\n",
    "#   print(\"Equal Opportunity Score :\",max_diff)\n",
    "#   print(table)\n",
    "  return max_diff\n",
    "\n",
    "def fair_Treatment_Equality (X,y_var,sensitive_var,prediction):\n",
    "#   print(\"----------------------------------------------------\")\n",
    "#   print(\"\\t\\t\\t\",sensitive_var)\n",
    "#   print('----------------------------------------------------')\n",
    "  groups = X[sensitive_var].unique()\n",
    "  tpr_scores = []\n",
    "  fpr_scores = []\n",
    "\n",
    "  # Create a PrettyTable object\n",
    "  table = PrettyTable()\n",
    "  table.field_names = [\"Name\", \"TPR\",'FPR']\n",
    "\n",
    "  for group in groups:\n",
    "    group_data = X[X[sensitive_var] == group]\n",
    "    y_true_group = group_data[y_var]\n",
    "    y_pred_group = group_data[prediction]\n",
    "\n",
    "    # Calculate True Positive Rate (Sensitivity)\n",
    "    cm = confusion_matrix(y_true_group, y_pred_group)\n",
    "    tpr = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # Calculate False Positive Rate\n",
    "    fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    # Append TPR and FPR for the current group\n",
    "    tpr_scores.append(tpr)\n",
    "    fpr_scores.append(fpr)\n",
    "\n",
    "    table.add_row([group, tpr,fpr])\n",
    "\n",
    "  # Find the maximum absolute difference in Proportion Positive across groups\n",
    "  max_diff_tpr = max(abs(tpr1 - tpr2) for tpr1, tpr2 in combinations(tpr_scores, 2))\n",
    "  max_diff_fpr = max(abs(fpr1 - fpr2) for fpr1, fpr2 in combinations(fpr_scores, 2))\n",
    "\n",
    "\n",
    "#   print(\"Treatment Equality Score :\",max(max_diff_tpr, max_diff_fpr))\n",
    "\n",
    "#   print(table)\n",
    "  return max(max_diff_tpr, max_diff_fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBANabZz3AsS"
   },
   "source": [
    "# **Equal sampling function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wjtOkXih2-kX"
   },
   "outputs": [],
   "source": [
    "def equal_sampling(dataframe, variables):\n",
    "    \"\"\"\n",
    "    Perform equal sampling from a pandas DataFrame based on a list of variables.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pandas DataFrame.\n",
    "    - variables: list of column names in the dataframe to be used for creating groups.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame with equal representation from each group formed by the unique combinations of the specified variables.\n",
    "    \"\"\"\n",
    "    # Generate all unique combinations of the specified variables\n",
    "    combinations = dataframe.groupby(variables).size().reset_index().rename(columns={0: 'count'})\n",
    "\n",
    "    # Determine the smallest group size among these combinations\n",
    "    min_size = combinations['count'].min()\n",
    "\n",
    "    # Initialize an empty DataFrame to store the sampled data\n",
    "    sampled_df = pd.DataFrame(columns=dataframe.columns)\n",
    "\n",
    "    # Loop through each unique combination, filter the dataframe, and sample\n",
    "    for _, row in combinations.iterrows():\n",
    "        filter_criteria = (dataframe[variables[0]] == row[variables[0]])\n",
    "        for var in variables[1:]:\n",
    "            filter_criteria &= (dataframe[var] == row[var])\n",
    "\n",
    "        sampled_group = dataframe[filter_criteria].sample(n=min_size, replace=True) # Use replace=True if min_size is larger than the group\n",
    "        sampled_df = pd.concat([sampled_df, sampled_group])\n",
    "\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JmgEhMo3YuC"
   },
   "source": [
    "# **Import the dataset and read**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1SwvHFrwO8MzWkUaVvBwy90YJFKk7bKsv\n",
      "From (redirected): https://drive.google.com/uc?id=1SwvHFrwO8MzWkUaVvBwy90YJFKk7bKsv&confirm=t&uuid=7d0597bb-1df4-4e4c-8d88-86acba3ec2ae\n",
      "To: /home/sagemaker-user/a,b,r optimization/filename.csv\n",
      "100%|██████████| 294M/294M [00:14<00:00, 20.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: lnrwg, Value: 0.6723413437409433\n",
      "Index: un_lnrealwg, Value: 0.6723413437409433\n",
      "Index: annhrs, Value: 0.33656719471190927\n",
      "Index: o_educ99, Value: 0.321437606751329\n",
      "Index: educ99, Value: 0.32143760675132893\n",
      "Index: o_educ, Value: 0.3210114679139357\n",
      "Index: educorig, Value: 0.3210114679139357\n",
      "Index: sch, Value: 0.32087980762914753\n",
      "Index: o_uhrswork, Value: 0.30280887755616137\n",
      "Index: uhrswork, Value: 0.30280887755616126\n",
      "Index: year, Value: 0.28099251338344694\n",
      "Index: perconexp, Value: 0.28057546088575414\n",
      "Index: adv, Value: 0.2666275677625677\n",
      "Index: hrswork, Value: 0.24374865893879202\n",
      "Index: lawyerphysician, Value: 0.21953506148083038\n",
      "Index: o_hrswork, Value: 0.2137403223720228\n",
      "Index: manager, Value: 0.2022461168027743\n",
      "Index: ft, Value: 0.2002081382833105\n",
      "Index: o_wkswork2, Value: 0.19643748929302945\n",
      "Index: wkswork2, Value: 0.1964374892930294\n",
      "Index: wkswork1, Value: 0.19090715505046507\n",
      "Index: o_wkswork1, Value: 0.19090715505046502\n",
      "Index: ind_orig, Value: 0.19064950265246336\n",
      "Index: o_ind, Value: 0.19064950265246336\n",
      "Index: ind, Value: 0.1906495026524633\n",
      "Index: o_indly, Value: 0.18931196091562938\n",
      "Index: indly, Value: 0.18931196091562935\n",
      "Index: ba, Value: 0.14510414730555893\n",
      "Index: oincwage, Value: 0.1258864051930252\n",
      "Index: o_oincwage, Value: 0.1258864017299741\n",
      "Index: o_age, Value: 0.12213716814095847\n",
      "Index: age, Value: 0.12213716814095842\n",
      "Index: o_metro, Value: 0.1132993245586576\n",
      "Index: computer, Value: 0.10108310333253144\n",
      "Index: tcoincwage, Value: 0.09976895616922973\n",
      "Index: metro, Value: 0.09304720071821161\n",
      "Index: hdwfcoh, Value: 0.09126995448960959\n",
      "Index: professional, Value: 0.08774616858503823\n",
      "Index: white, Value: 0.07735490184941504\n",
      "Index: architect, Value: 0.07535026755270875\n",
      "Index: o_schlcoll, Value: 0.07263710670507048\n",
      "Index: finance, Value: 0.06758011104037402\n",
      "Index: financialop, Value: 0.05777625501191781\n",
      "Index: schlcoll, Value: 0.0523990248384521\n",
      "Index: potexp, Value: 0.0517324560276037\n",
      "Index: publicadmin, Value: 0.04609900332362174\n",
      "Index: hrwage, Value: 0.04315019663456549\n",
      "Index: scientist, Value: 0.04233160401442411\n",
      "Index: realhrwage, Value: 0.04185512662097031\n",
      "Index: uncenrealhrwage, Value: 0.04185512662097031\n",
      "Index: business, Value: 0.04047638449391455\n",
      "Index: northeast, Value: 0.038544340806243485\n",
      "Index: Communications, Value: 0.03595521237993464\n",
      "Index: wagesamp, Value: 0.03420752059675046\n",
      "Index: o_county, Value: 0.03352483408783848\n",
      "Index: othrace, Value: 0.031063662938790426\n",
      "Index: potexp2, Value: 0.029465290637066557\n",
      "Index: Utilities, Value: 0.028238255844078696\n",
      "Index: healthcare, Value: 0.02664376688799964\n",
      "Index: serial, Value: 0.02341835331321801\n",
      "Index: o_race, Value: 0.020680189331558185\n",
      "Index: origrace, Value: 0.020680189331558185\n",
      "Index: o_hwtsupp, Value: 0.020368274009958547\n",
      "Index: hwtsupp, Value: 0.02036827400995852\n",
      "Index: ind1990, Value: 0.019981412199116304\n",
      "Index: o_ind50ly, Value: 0.019937515476673776\n",
      "Index: ind1950, Value: 0.01982735498504595\n",
      "Index: o_ind1950, Value: 0.019827354985045897\n",
      "Index: ind50ly, Value: 0.01975339039269532\n",
      "Index: postseceduc, Value: 0.019578033592421083\n",
      "Index: metarea, Value: 0.01937663771230744\n",
      "Index: o_ind1990, Value: 0.01911702237605424\n",
      "Index: protective, Value: 0.018461353449727838\n",
      "Index: durables, Value: 0.01734896074549748\n",
      "Index: adj_ind2, Value: 0.014809071668598236\n",
      "Index: mbpl, Value: 0.014707299667395719\n",
      "Index: o_mbpl, Value: 0.01336315516367995\n",
      "Index: fbpl, Value: 0.013236786949639655\n",
      "Index: artist, Value: 0.013012873678380863\n",
      "Index: wholesaletrade, Value: 0.012921626339624938\n",
      "Index: o_wtsupp, Value: 0.01204603073765565\n",
      "Index: wtsupp, Value: 0.012046030737655644\n",
      "Index: o_fbpl, Value: 0.011754049207817238\n",
      "Index: county, Value: 0.008325646273009964\n",
      "Index: classwkr, Value: 0.008278253994872876\n",
      "Index: o_classwkr, Value: 0.00827825399487287\n",
      "Index: Medical, Value: 0.006091188339821148\n",
      "Index: miningconstruction, Value: 0.006086773860107565\n",
      "Index: west, Value: 0.005541088833198222\n",
      "Index: Transport, Value: 0.0017825390255255386\n",
      "Index: bpl, Value: 0.0014759207891889474\n",
      "Index: o_bpl, Value: 0.0009305480755941709\n",
      "Index: sales, Value: -0.002019517726789147\n",
      "Index: o_occ, Value: -0.004135524257821278\n",
      "Index: occ_orig, Value: -0.004135524257821278\n",
      "Index: occ, Value: -0.004135524257821279\n",
      "Index: occly, Value: -0.0044535558782538886\n",
      "Index: o_occly, Value: -0.004453555878253905\n",
      "Index: adj_ind, Value: -0.0060080119258539705\n",
      "Index: o_numprec, Value: -0.006399395201980594\n",
      "Index: numprec, Value: -0.006399395201980603\n",
      "Index: o_srcearn, Value: -0.008012373463064501\n",
      "Index: srcearn, Value: -0.008012373463064501\n",
      "Index: socialworker, Value: -0.011026777295318282\n",
      "Index: o_union, Value: -0.013088576103522185\n",
      "Index: union, Value: -0.013088576103522192\n",
      "Index: o_empstat, Value: -0.015230043196130196\n",
      "Index: empstat, Value: -0.015230043196130198\n",
      "Index: nondurables, Value: -0.015294684940952898\n",
      "Index: constructextractinstall, Value: -0.01641107648980605\n",
      "Index: ind_1999, Value: -0.016936120435990586\n",
      "Index: qincfarm, Value: -0.018580654134356\n",
      "Index: o_qincfarm, Value: -0.018580654134356\n",
      "Index: o_qincbus, Value: -0.018629962476543926\n",
      "Index: qincbus, Value: -0.018629962476543926\n",
      "Index: o_region, Value: -0.019859463859635943\n",
      "Index: region, Value: -0.019859463859635947\n",
      "Index: south, Value: -0.020195536579752767\n",
      "Index: northcentral, Value: -0.02033697948190115\n",
      "Index: qwkswork, Value: -0.021932520512482068\n",
      "Index: o_qwkswork, Value: -0.021932520512482068\n",
      "Index: ind_2002_orig, Value: -0.023277230335284168\n",
      "Index: quhrswor, Value: -0.02373242648396985\n",
      "Index: o_quhrswor, Value: -0.02373242648396985\n",
      "Index: classwly, Value: -0.024560255462973263\n",
      "Index: o_classwly, Value: -0.024560255462973274\n",
      "Index: statefip, Value: -0.028523922537622287\n",
      "Index: o_statefip, Value: -0.0285239225376223\n",
      "Index: ind_2007_orig, Value: -0.03182446880241572\n",
      "Index: Education, Value: -0.031984098297842094\n",
      "Index: Agriculture, Value: -0.03265011586487532\n",
      "Index: legaleduc, Value: -0.0333076460222229\n",
      "Index: farmer, Value: -0.036686644344413814\n",
      "Index: ind2000_99, Value: -0.042885168723295904\n",
      "Index: ftype, Value: -0.04464822604410457\n",
      "Index: o_ftype, Value: -0.04464822604410457\n",
      "Index: black, Value: -0.04626904178919838\n",
      "Index: transport, Value: -0.049161107498335646\n",
      "Index: o_nativity, Value: -0.05036831617147627\n",
      "Index: nativity, Value: -0.05059667787861805\n",
      "Index: race, Value: -0.05235880902669476\n",
      "Index: o_farm, Value: -0.0525065580727609\n",
      "Index: farm, Value: -0.05250655807276091\n",
      "Index: o_yrimmig, Value: -0.05872247197576554\n",
      "Index: retailtrade, Value: -0.06537352700822761\n",
      "Index: healthsupport, Value: -0.06541290714413858\n",
      "Index: relate, Value: -0.0669719500106459\n",
      "Index: o_relate, Value: -0.06697195001064592\n",
      "Index: o_hispan, Value: -0.0670621394808569\n",
      "Index: SocArtOther, Value: -0.06917522082802344\n",
      "Index: o_citizen, Value: -0.06942578986555625\n",
      "Index: production, Value: -0.07380752529032898\n",
      "Index: pernum, Value: -0.07413582693459206\n",
      "Index: o_pernum, Value: -0.07413582693459206\n",
      "Index: ind_1990, Value: -0.07576750807263123\n",
      "Index: yrimmig, Value: -0.0768193322468192\n",
      "Index: hisp, Value: -0.08393571035311359\n",
      "Index: hispan, Value: -0.08479005652023452\n",
      "Index: building, Value: -0.08589634763003624\n",
      "Index: o_marst, Value: -0.08706066513372407\n",
      "Index: marst, Value: -0.08706066513372407\n",
      "Index: hotelsrestaurants, Value: -0.08959375138878686\n",
      "Index: o_metarea, Value: -0.09769290926148196\n",
      "Index: ind2000_90, Value: -0.10216321796847322\n",
      "Index: officeadmin, Value: -0.1082079242089689\n",
      "Index: occ_2000female, Value: -0.11648339541054663\n",
      "Index: foodcare, Value: -0.11966932487301526\n",
      "Index: citizen, Value: -0.144478832794067\n",
      "Index: ind_81, Value: -0.14677297010623175\n",
      "Index: ind2000_81, Value: -0.14874064587579608\n",
      "Index: ind_2000, Value: -0.14874064587579608\n",
      "Index: female, Value: -0.17980031042226635\n",
      "Index: sex, Value: -0.17980031042226655\n",
      "Index: o_sex, Value: -0.1798003104222666\n",
      "Index: occ_2000male, Value: -0.1892237497810216\n",
      "Index: occ2000_81, Value: -0.1966005669698369\n",
      "Index: occ1950, Value: -0.22926394661901522\n",
      "Index: o_occ1950, Value: -0.22926394661901525\n",
      "Index: o_occ50ly, Value: -0.23053787263586017\n",
      "Index: occ50ly, Value: -0.23053787263586017\n",
      "Index: occ2000_99, Value: -0.2309922359681891\n",
      "Index: adj_occ2, Value: -0.23219258759560796\n",
      "Index: occ_2010_orig, Value: -0.23810775845034857\n",
      "Index: adj_occ, Value: -0.2389403103349855\n",
      "Index: occ2000_90, Value: -0.24374050770068786\n",
      "Index: o_occ1990, Value: -0.24594063986430964\n",
      "Index: occ1990, Value: -0.24594063986430967\n",
      "Index: occ_1999, Value: -0.2529393573295136\n",
      "Index: occ_1990, Value: -0.2619065866503259\n",
      "Index: inflate, Value: -0.2684801397012578\n",
      "Index: LEHS, Value: -0.3061936793976086\n",
      "Index: occ_81, Value: -0.30935988267407205\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gdown\n",
    "url = 'https://drive.google.com/uc?id=1SwvHFrwO8MzWkUaVvBwy90YJFKk7bKsv'\n",
    "output = 'filename.csv'  # You can rename 'filename.csv' to the actual name of your file\n",
    "gdown.download(url, output, quiet=False)\n",
    "full_dataset = pd.read_csv(output)\n",
    "\n",
    "numeric_df = full_dataset.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "correlation_matrix = numeric_df.corr()  # Compute the correlation matrix\n",
    "\n",
    "# Get the correlations with 'incwage' and drop the self-correlation\n",
    "incwage_correlations = correlation_matrix['incwage'].drop('incwage')\n",
    "\n",
    "# Sort the correlations\n",
    "sorted_correlations = incwage_correlations.sort_values(ascending=False)\n",
    "cols=[]\n",
    "# Assuming 'series' is your Pandas Series\n",
    "for index, value in sorted_correlations.items():\n",
    "    if value<0.7 and value>-1:\n",
    "        print(f\"Index: {index}, Value: {value}\")\n",
    "        cols+=[index]\n",
    "cols+=['incwage']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "o_sex\n",
      "race\n",
      "o_race\n",
      "origrace\n",
      "hisp\n",
      "hispan\n",
      "o_hispan\n",
      "black\n",
      "female\n",
      "marst\n",
      "o_marst\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lnrwg</th>\n",
       "      <th>un_lnrealwg</th>\n",
       "      <th>annhrs</th>\n",
       "      <th>o_educ99</th>\n",
       "      <th>educ99</th>\n",
       "      <th>o_educ</th>\n",
       "      <th>educorig</th>\n",
       "      <th>sch</th>\n",
       "      <th>o_uhrswork</th>\n",
       "      <th>uhrswork</th>\n",
       "      <th>...</th>\n",
       "      <th>occ_2010_orig</th>\n",
       "      <th>adj_occ</th>\n",
       "      <th>occ2000_90</th>\n",
       "      <th>o_occ1990</th>\n",
       "      <th>occ1990</th>\n",
       "      <th>occ_1999</th>\n",
       "      <th>occ_1990</th>\n",
       "      <th>inflate</th>\n",
       "      <th>LEHS</th>\n",
       "      <th>occ_81</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.507147</td>\n",
       "      <td>2.507147</td>\n",
       "      <td>1820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>526.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>335.0</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.155841</td>\n",
       "      <td>2.155841</td>\n",
       "      <td>2080</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>337</td>\n",
       "      <td>337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.015900</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.052578</td>\n",
       "      <td>3.052578</td>\n",
       "      <td>2080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>14.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217.0</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.017873</td>\n",
       "      <td>3.017873</td>\n",
       "      <td>2115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.938754</td>\n",
       "      <td>2.938754</td>\n",
       "      <td>2080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>1</td>\n",
       "      <td>245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344282</th>\n",
       "      <td>2.820971</td>\n",
       "      <td>2.820971</td>\n",
       "      <td>2340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>0</td>\n",
       "      <td>174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344283</th>\n",
       "      <td>2.425176</td>\n",
       "      <td>2.425176</td>\n",
       "      <td>360</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.271724</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344284</th>\n",
       "      <td>2.670490</td>\n",
       "      <td>2.670490</td>\n",
       "      <td>2080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>470.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>1</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344285</th>\n",
       "      <td>3.631611</td>\n",
       "      <td>3.631611</td>\n",
       "      <td>250</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>286.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.073115</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344286</th>\n",
       "      <td>2.444559</td>\n",
       "      <td>2.444559</td>\n",
       "      <td>1248</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>4760.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.958919</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344287 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lnrwg  un_lnrealwg  annhrs  o_educ99  educ99  o_educ  educorig  \\\n",
       "0       2.507147     2.507147    1820       NaN     NaN      72        72   \n",
       "1       2.155841     2.155841    2080      10.0    10.0      73        73   \n",
       "2       3.052578     3.052578    2080       NaN     NaN     100       100   \n",
       "3       3.017873     3.017873    2115       NaN     NaN     122       122   \n",
       "4       2.938754     2.938754    2080       NaN     NaN      90        90   \n",
       "...          ...          ...     ...       ...     ...     ...       ...   \n",
       "344282  2.820971     2.820971    2340       NaN     NaN     122       122   \n",
       "344283  2.425176     2.425176     360      10.0    10.0      73        73   \n",
       "344284  2.670490     2.670490    2080       NaN     NaN      50        50   \n",
       "344285  3.631611     3.631611     250      18.0    18.0     125       125   \n",
       "344286  2.444559     2.444559    1248      11.0    11.0      81        81   \n",
       "\n",
       "         sch  o_uhrswork  uhrswork  ...  occ_2010_orig  adj_occ  occ2000_90  \\\n",
       "0       12.0          35        35  ...            NaN    526.0       526.0   \n",
       "1       12.0          40        40  ...            NaN    512.0         NaN   \n",
       "2       14.0          40        40  ...            NaN    154.0       154.0   \n",
       "3       18.0          45        45  ...            NaN    102.0       102.0   \n",
       "4       13.0          40        40  ...            NaN     43.0         NaN   \n",
       "...      ...         ...       ...  ...            ...      ...         ...   \n",
       "344282  18.0          45        45  ...            NaN    200.0         NaN   \n",
       "344283  12.0          40        40  ...            NaN      5.0         NaN   \n",
       "344284  10.0          40        40  ...            NaN    470.0         NaN   \n",
       "344285  18.0          10        10  ...            NaN    286.0         NaN   \n",
       "344286  13.0          24        24  ...         4760.0    476.0         NaN   \n",
       "\n",
       "        o_occ1990  occ1990  occ_1999  occ_1990   inflate  LEHS  occ_81  \n",
       "0             335      335       NaN     335.0  1.572618     1     NaN  \n",
       "1             337      337       NaN       NaN  1.015900     1     NaN  \n",
       "2             217      217       NaN     217.0  1.572618     1     NaN  \n",
       "3              64       64       NaN      64.0  1.572618     0     NaN  \n",
       "4              22       22       NaN       NaN  2.311526     1   245.0  \n",
       "...           ...      ...       ...       ...       ...   ...     ...  \n",
       "344282        163      163       NaN       NaN  2.311526     0   174.0  \n",
       "344283         13       13      13.0       NaN  1.271724     1     NaN  \n",
       "344284         13       13       NaN       NaN  2.311526     1   231.0  \n",
       "344285        194      194       NaN       NaN  1.073115     0     NaN  \n",
       "344286        275      275       NaN       NaN  0.958919     1     NaN  \n",
       "\n",
       "[344287 rows x 180 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "output = 'filename.csv'  # You can rename 'filename.csv' to the actual name of your file\n",
    "df = pd.read_csv(output)\n",
    "df= df.loc[:,cols]\n",
    "sensitive_variables = ['sex', 'o_sex',\n",
    "                            'race', 'o_race', 'origrace',\n",
    "                            'hisp', 'hispan', 'o_hispan',\n",
    "                            'black',\n",
    "                            'female',\n",
    "                            'marst', 'o_marst'\n",
    "                        ]\n",
    "for I in sensitive_variables:\n",
    "    if I in df.columns:\n",
    "        print(I)\n",
    "    else:\n",
    "        print(\"no\",I)\n",
    "\n",
    "# Separate the target column\n",
    "df_y = df.loc[:,['incwage']]\n",
    "df = df.drop(['incwage'], axis=1)\n",
    "df_sen = df.loc[:,['sex']]\n",
    "df = df.drop(['sex'], axis=1)\n",
    "\n",
    "sensitive_variables = [ 'o_sex',\n",
    "                            'race', 'o_race', 'origrace',\n",
    "                            'hisp', 'hispan', 'o_hispan',\n",
    "                            'black',\n",
    "                            'female',\n",
    "                            'marst', 'o_marst'\n",
    "                        ]\n",
    "df = df.drop(sensitive_variables, axis=1)\n",
    "\n",
    "# Identify categorical columns with more than 2 unique values\n",
    "categorical_cols = df.select_dtypes(include=['category']).columns\n",
    "categorical_cols_more_than_two = [col for col in categorical_cols if len(df[col].unique()) > 2]\n",
    "# Initialize OneHotEncoder to drop the first category of each feature to avoid multicollinearity\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "# Apply OneHotEncoder to the categorical columns\n",
    "encoded_data = encoder.fit_transform(df[categorical_cols_more_than_two])\n",
    "# Create a DataFrame from the encoded data with column names from the encoder\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(), \n",
    "                          columns=encoder.get_feature_names_out(categorical_cols_more_than_two))\n",
    "# Drop the original categorical columns from the original DataFrame\n",
    "df = df.drop(columns=categorical_cols_more_than_two)\n",
    "# Concatenate the original DataFrame with the new encoded DataFrame\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incwage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344282</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344283</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344284</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344285</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344286</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344287 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        incwage\n",
       "0             0\n",
       "1             0\n",
       "2             0\n",
       "3             0\n",
       "4             0\n",
       "...         ...\n",
       "344282        0\n",
       "344283        0\n",
       "344284        0\n",
       "344285        0\n",
       "344286        0\n",
       "\n",
       "[344287 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y['incwage'] = (df_y['incwage'] > 50000).astype(int)\n",
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344282</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344283</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344284</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344285</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344286</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344287 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sex\n",
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "...     ...\n",
       "344282    1\n",
       "344283    1\n",
       "344284    1\n",
       "344285    1\n",
       "344286    1\n",
       "\n",
       "[344287 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sen\n",
    "df_sen_temp=dict()\n",
    "replacement_mapping = { 1:0, 2:1}\n",
    "df_sen_temp['sex'] = df_sen['sex'].map(replacement_mapping).to_numpy().astype(\"int\")\n",
    "df_sen = pd.DataFrame(df_sen_temp)\n",
    "df_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lnrwg</th>\n",
       "      <th>un_lnrealwg</th>\n",
       "      <th>annhrs</th>\n",
       "      <th>o_educ</th>\n",
       "      <th>educorig</th>\n",
       "      <th>sch</th>\n",
       "      <th>o_uhrswork</th>\n",
       "      <th>uhrswork</th>\n",
       "      <th>year</th>\n",
       "      <th>perconexp</th>\n",
       "      <th>...</th>\n",
       "      <th>occ1950</th>\n",
       "      <th>o_occ1950</th>\n",
       "      <th>o_occ50ly</th>\n",
       "      <th>occ50ly</th>\n",
       "      <th>adj_occ2</th>\n",
       "      <th>adj_occ</th>\n",
       "      <th>o_occ1990</th>\n",
       "      <th>occ1990</th>\n",
       "      <th>inflate</th>\n",
       "      <th>LEHS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.507147</td>\n",
       "      <td>2.507147</td>\n",
       "      <td>1820</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>1990</td>\n",
       "      <td>64.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>42</td>\n",
       "      <td>526.0</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.155841</td>\n",
       "      <td>2.155841</td>\n",
       "      <td>2080</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>2009</td>\n",
       "      <td>100.063004</td>\n",
       "      <td>...</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>42</td>\n",
       "      <td>512.0</td>\n",
       "      <td>337</td>\n",
       "      <td>337</td>\n",
       "      <td>1.015900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.052578</td>\n",
       "      <td>3.052578</td>\n",
       "      <td>2080</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>14.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1990</td>\n",
       "      <td>64.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>154.0</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.017873</td>\n",
       "      <td>3.017873</td>\n",
       "      <td>2115</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>1990</td>\n",
       "      <td>64.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>20</td>\n",
       "      <td>102.0</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.938754</td>\n",
       "      <td>2.938754</td>\n",
       "      <td>2080</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1981</td>\n",
       "      <td>43.977001</td>\n",
       "      <td>...</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>10</td>\n",
       "      <td>43.0</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344282</th>\n",
       "      <td>2.820971</td>\n",
       "      <td>2.820971</td>\n",
       "      <td>2340</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>1981</td>\n",
       "      <td>43.977001</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>23</td>\n",
       "      <td>200.0</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344283</th>\n",
       "      <td>2.425176</td>\n",
       "      <td>2.425176</td>\n",
       "      <td>360</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1999</td>\n",
       "      <td>79.933998</td>\n",
       "      <td>...</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.271724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344284</th>\n",
       "      <td>2.670490</td>\n",
       "      <td>2.670490</td>\n",
       "      <td>2080</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1981</td>\n",
       "      <td>43.977001</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>470.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344285</th>\n",
       "      <td>3.631611</td>\n",
       "      <td>3.631611</td>\n",
       "      <td>250</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2007</td>\n",
       "      <td>94.727997</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>286.0</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "      <td>1.073115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344286</th>\n",
       "      <td>2.444559</td>\n",
       "      <td>2.444559</td>\n",
       "      <td>1248</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>2013</td>\n",
       "      <td>106.009003</td>\n",
       "      <td>...</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>40</td>\n",
       "      <td>476.0</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>0.958919</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344287 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lnrwg  un_lnrealwg  annhrs  o_educ  educorig   sch  o_uhrswork  \\\n",
       "0       2.507147     2.507147    1820      72        72  12.0          35   \n",
       "1       2.155841     2.155841    2080      73        73  12.0          40   \n",
       "2       3.052578     3.052578    2080     100       100  14.0          40   \n",
       "3       3.017873     3.017873    2115     122       122  18.0          45   \n",
       "4       2.938754     2.938754    2080      90        90  13.0          40   \n",
       "...          ...          ...     ...     ...       ...   ...         ...   \n",
       "344282  2.820971     2.820971    2340     122       122  18.0          45   \n",
       "344283  2.425176     2.425176     360      73        73  12.0          40   \n",
       "344284  2.670490     2.670490    2080      50        50  10.0          40   \n",
       "344285  3.631611     3.631611     250     125       125  18.0          10   \n",
       "344286  2.444559     2.444559    1248      81        81  13.0          24   \n",
       "\n",
       "        uhrswork  year   perconexp  ...  occ1950  o_occ1950  o_occ50ly  \\\n",
       "0             35  1990   64.639999  ...      390        390        390   \n",
       "1             40  2009  100.063004  ...      310        310        310   \n",
       "2             40  1990   64.639999  ...       35         35         35   \n",
       "3             45  1990   64.639999  ...       99         99         99   \n",
       "4             40  1981   43.977001  ...      290        290         95   \n",
       "...          ...   ...         ...  ...      ...        ...        ...   \n",
       "344282        45  1981   43.977001  ...       99         99         99   \n",
       "344283        40  1999   79.933998  ...      201        201        201   \n",
       "344284        40  1981   43.977001  ...      200        200        200   \n",
       "344285        10  2007   94.727997  ...       99         99         29   \n",
       "344286        24  2013  106.009003  ...      490        490        490   \n",
       "\n",
       "        occ50ly  adj_occ2  adj_occ  o_occ1990  occ1990   inflate  LEHS  \n",
       "0           390        42    526.0        335      335  1.572618     1  \n",
       "1           310        42    512.0        337      337  1.015900     1  \n",
       "2            35        21    154.0        217      217  1.572618     1  \n",
       "3            99        20    102.0         64       64  1.572618     0  \n",
       "4            95        10     43.0         22       22  2.311526     1  \n",
       "...         ...       ...      ...        ...      ...       ...   ...  \n",
       "344282       99        23    200.0        163      163  2.311526     0  \n",
       "344283      201        10      5.0         13       13  1.271724     1  \n",
       "344284      200        40    470.0         13       13  2.311526     1  \n",
       "344285       29        27    286.0        194      194  1.073115     0  \n",
       "344286      490        40    476.0        275      275  0.958919     1  \n",
       "\n",
       "[344287 rows x 126 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_count = df.isna().sum()\n",
    "columns_with_no_missing_values = missing_values_count[missing_values_count == 0].index\n",
    "df_no_missing = df.loc[:,list(columns_with_no_missing_values)]\n",
    "df_no_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lnrwg</th>\n",
       "      <th>un_lnrealwg</th>\n",
       "      <th>annhrs</th>\n",
       "      <th>o_educ</th>\n",
       "      <th>educorig</th>\n",
       "      <th>sch</th>\n",
       "      <th>o_uhrswork</th>\n",
       "      <th>uhrswork</th>\n",
       "      <th>year</th>\n",
       "      <th>perconexp</th>\n",
       "      <th>...</th>\n",
       "      <th>occ1950</th>\n",
       "      <th>o_occ1950</th>\n",
       "      <th>o_occ50ly</th>\n",
       "      <th>occ50ly</th>\n",
       "      <th>adj_occ2</th>\n",
       "      <th>adj_occ</th>\n",
       "      <th>o_occ1990</th>\n",
       "      <th>occ1990</th>\n",
       "      <th>inflate</th>\n",
       "      <th>LEHS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.507147</td>\n",
       "      <td>2.507147</td>\n",
       "      <td>1820</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>1990</td>\n",
       "      <td>64.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>42</td>\n",
       "      <td>526.0</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.155841</td>\n",
       "      <td>2.155841</td>\n",
       "      <td>2080</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>2009</td>\n",
       "      <td>100.063004</td>\n",
       "      <td>...</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>42</td>\n",
       "      <td>512.0</td>\n",
       "      <td>337</td>\n",
       "      <td>337</td>\n",
       "      <td>1.015900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.052578</td>\n",
       "      <td>3.052578</td>\n",
       "      <td>2080</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>14.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1990</td>\n",
       "      <td>64.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>154.0</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.017873</td>\n",
       "      <td>3.017873</td>\n",
       "      <td>2115</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>1990</td>\n",
       "      <td>64.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>20</td>\n",
       "      <td>102.0</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.938754</td>\n",
       "      <td>2.938754</td>\n",
       "      <td>2080</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1981</td>\n",
       "      <td>43.977001</td>\n",
       "      <td>...</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>10</td>\n",
       "      <td>43.0</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344282</th>\n",
       "      <td>2.820971</td>\n",
       "      <td>2.820971</td>\n",
       "      <td>2340</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>1981</td>\n",
       "      <td>43.977001</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>23</td>\n",
       "      <td>200.0</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344283</th>\n",
       "      <td>2.425176</td>\n",
       "      <td>2.425176</td>\n",
       "      <td>360</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1999</td>\n",
       "      <td>79.933998</td>\n",
       "      <td>...</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.271724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344284</th>\n",
       "      <td>2.670490</td>\n",
       "      <td>2.670490</td>\n",
       "      <td>2080</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1981</td>\n",
       "      <td>43.977001</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>470.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344285</th>\n",
       "      <td>3.631611</td>\n",
       "      <td>3.631611</td>\n",
       "      <td>250</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2007</td>\n",
       "      <td>94.727997</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>286.0</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "      <td>1.073115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344286</th>\n",
       "      <td>2.444559</td>\n",
       "      <td>2.444559</td>\n",
       "      <td>1248</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>2013</td>\n",
       "      <td>106.009003</td>\n",
       "      <td>...</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>40</td>\n",
       "      <td>476.0</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>0.958919</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344287 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lnrwg  un_lnrealwg  annhrs  o_educ  educorig   sch  o_uhrswork  \\\n",
       "0       2.507147     2.507147    1820      72        72  12.0          35   \n",
       "1       2.155841     2.155841    2080      73        73  12.0          40   \n",
       "2       3.052578     3.052578    2080     100       100  14.0          40   \n",
       "3       3.017873     3.017873    2115     122       122  18.0          45   \n",
       "4       2.938754     2.938754    2080      90        90  13.0          40   \n",
       "...          ...          ...     ...     ...       ...   ...         ...   \n",
       "344282  2.820971     2.820971    2340     122       122  18.0          45   \n",
       "344283  2.425176     2.425176     360      73        73  12.0          40   \n",
       "344284  2.670490     2.670490    2080      50        50  10.0          40   \n",
       "344285  3.631611     3.631611     250     125       125  18.0          10   \n",
       "344286  2.444559     2.444559    1248      81        81  13.0          24   \n",
       "\n",
       "        uhrswork  year   perconexp  ...  occ1950  o_occ1950  o_occ50ly  \\\n",
       "0             35  1990   64.639999  ...      390        390        390   \n",
       "1             40  2009  100.063004  ...      310        310        310   \n",
       "2             40  1990   64.639999  ...       35         35         35   \n",
       "3             45  1990   64.639999  ...       99         99         99   \n",
       "4             40  1981   43.977001  ...      290        290         95   \n",
       "...          ...   ...         ...  ...      ...        ...        ...   \n",
       "344282        45  1981   43.977001  ...       99         99         99   \n",
       "344283        40  1999   79.933998  ...      201        201        201   \n",
       "344284        40  1981   43.977001  ...      200        200        200   \n",
       "344285        10  2007   94.727997  ...       99         99         29   \n",
       "344286        24  2013  106.009003  ...      490        490        490   \n",
       "\n",
       "        occ50ly  adj_occ2  adj_occ  o_occ1990  occ1990   inflate  LEHS  \n",
       "0           390        42    526.0        335      335  1.572618     1  \n",
       "1           310        42    512.0        337      337  1.015900     1  \n",
       "2            35        21    154.0        217      217  1.572618     1  \n",
       "3            99        20    102.0         64       64  1.572618     0  \n",
       "4            95        10     43.0         22       22  2.311526     1  \n",
       "...         ...       ...      ...        ...      ...       ...   ...  \n",
       "344282       99        23    200.0        163      163  2.311526     0  \n",
       "344283      201        10      5.0         13       13  1.271724     1  \n",
       "344284      200        40    470.0         13       13  2.311526     1  \n",
       "344285       29        27    286.0        194      194  1.073115     0  \n",
       "344286      490        40    476.0        275      275  0.958919     1  \n",
       "\n",
       "[344287 rows x 126 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X = df_no_missing\n",
    "df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOOTQiwQ3fsL"
   },
   "source": [
    "# **Assign variable categories**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_6H_qNxe7hd"
   },
   "source": [
    "# **Edit this cell acording to the varibale names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A1qLUJfWe8bV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lnrwg</th>\n",
       "      <th>un_lnrealwg</th>\n",
       "      <th>annhrs</th>\n",
       "      <th>o_educ</th>\n",
       "      <th>educorig</th>\n",
       "      <th>sch</th>\n",
       "      <th>o_uhrswork</th>\n",
       "      <th>uhrswork</th>\n",
       "      <th>year</th>\n",
       "      <th>perconexp</th>\n",
       "      <th>...</th>\n",
       "      <th>o_occ50ly</th>\n",
       "      <th>occ50ly</th>\n",
       "      <th>adj_occ2</th>\n",
       "      <th>adj_occ</th>\n",
       "      <th>o_occ1990</th>\n",
       "      <th>occ1990</th>\n",
       "      <th>inflate</th>\n",
       "      <th>LEHS</th>\n",
       "      <th>Sensitive</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.507147</td>\n",
       "      <td>2.507147</td>\n",
       "      <td>1820</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>1990</td>\n",
       "      <td>64.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>42</td>\n",
       "      <td>526.0</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.155841</td>\n",
       "      <td>2.155841</td>\n",
       "      <td>2080</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>2009</td>\n",
       "      <td>100.063004</td>\n",
       "      <td>...</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>42</td>\n",
       "      <td>512.0</td>\n",
       "      <td>337</td>\n",
       "      <td>337</td>\n",
       "      <td>1.015900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.052578</td>\n",
       "      <td>3.052578</td>\n",
       "      <td>2080</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>14.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1990</td>\n",
       "      <td>64.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>154.0</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.017873</td>\n",
       "      <td>3.017873</td>\n",
       "      <td>2115</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>1990</td>\n",
       "      <td>64.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>20</td>\n",
       "      <td>102.0</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>1.572618</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.938754</td>\n",
       "      <td>2.938754</td>\n",
       "      <td>2080</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1981</td>\n",
       "      <td>43.977001</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>10</td>\n",
       "      <td>43.0</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344282</th>\n",
       "      <td>2.820971</td>\n",
       "      <td>2.820971</td>\n",
       "      <td>2340</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>1981</td>\n",
       "      <td>43.977001</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>23</td>\n",
       "      <td>200.0</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344283</th>\n",
       "      <td>2.425176</td>\n",
       "      <td>2.425176</td>\n",
       "      <td>360</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1999</td>\n",
       "      <td>79.933998</td>\n",
       "      <td>...</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.271724</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344284</th>\n",
       "      <td>2.670490</td>\n",
       "      <td>2.670490</td>\n",
       "      <td>2080</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1981</td>\n",
       "      <td>43.977001</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>470.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2.311526</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344285</th>\n",
       "      <td>3.631611</td>\n",
       "      <td>3.631611</td>\n",
       "      <td>250</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2007</td>\n",
       "      <td>94.727997</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>286.0</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "      <td>1.073115</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344286</th>\n",
       "      <td>2.444559</td>\n",
       "      <td>2.444559</td>\n",
       "      <td>1248</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>2013</td>\n",
       "      <td>106.009003</td>\n",
       "      <td>...</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>40</td>\n",
       "      <td>476.0</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>0.958919</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344287 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lnrwg  un_lnrealwg  annhrs  o_educ  educorig   sch  o_uhrswork  \\\n",
       "0       2.507147     2.507147    1820      72        72  12.0          35   \n",
       "1       2.155841     2.155841    2080      73        73  12.0          40   \n",
       "2       3.052578     3.052578    2080     100       100  14.0          40   \n",
       "3       3.017873     3.017873    2115     122       122  18.0          45   \n",
       "4       2.938754     2.938754    2080      90        90  13.0          40   \n",
       "...          ...          ...     ...     ...       ...   ...         ...   \n",
       "344282  2.820971     2.820971    2340     122       122  18.0          45   \n",
       "344283  2.425176     2.425176     360      73        73  12.0          40   \n",
       "344284  2.670490     2.670490    2080      50        50  10.0          40   \n",
       "344285  3.631611     3.631611     250     125       125  18.0          10   \n",
       "344286  2.444559     2.444559    1248      81        81  13.0          24   \n",
       "\n",
       "        uhrswork  year   perconexp  ...  o_occ50ly  occ50ly  adj_occ2  \\\n",
       "0             35  1990   64.639999  ...        390      390        42   \n",
       "1             40  2009  100.063004  ...        310      310        42   \n",
       "2             40  1990   64.639999  ...         35       35        21   \n",
       "3             45  1990   64.639999  ...         99       99        20   \n",
       "4             40  1981   43.977001  ...         95       95        10   \n",
       "...          ...   ...         ...  ...        ...      ...       ...   \n",
       "344282        45  1981   43.977001  ...         99       99        23   \n",
       "344283        40  1999   79.933998  ...        201      201        10   \n",
       "344284        40  1981   43.977001  ...        200      200        40   \n",
       "344285        10  2007   94.727997  ...         29       29        27   \n",
       "344286        24  2013  106.009003  ...        490      490        40   \n",
       "\n",
       "        adj_occ  o_occ1990  occ1990   inflate  LEHS  Sensitive  Class  \n",
       "0         526.0        335      335  1.572618     1          0      0  \n",
       "1         512.0        337      337  1.015900     1          0      0  \n",
       "2         154.0        217      217  1.572618     1          0      0  \n",
       "3         102.0         64       64  1.572618     0          0      0  \n",
       "4          43.0         22       22  2.311526     1          0      0  \n",
       "...         ...        ...      ...       ...   ...        ...    ...  \n",
       "344282    200.0        163      163  2.311526     0          1      0  \n",
       "344283      5.0         13       13  1.271724     1          1      0  \n",
       "344284    470.0         13       13  2.311526     1          1      0  \n",
       "344285    286.0        194      194  1.073115     0          1      0  \n",
       "344286    476.0        275      275  0.958919     1          1      0  \n",
       "\n",
       "[344287 rows x 128 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X = df_X.dropna()\n",
    "ohe_full_dataset = df_X.reset_index(drop=True)\n",
    "ohe_full_dataset[\"Sensitive\"] = df_sen['sex']\n",
    "ohe_full_dataset[\"Class\"] = df_y['incwage']\n",
    "ohe_full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Id6nuVjo45JL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Dataset_name</th>\n",
       "      <th>Alpha-pred</th>\n",
       "      <th>Beta-Fair</th>\n",
       "      <th>Gamma-privacy</th>\n",
       "      <th>cost_multiplier-all</th>\n",
       "      <th>Balance_Class</th>\n",
       "      <th>Balance_Sensitive</th>\n",
       "      <th>Fairness_cost_function</th>\n",
       "      <th>Dataset_training_size</th>\n",
       "      <th>...</th>\n",
       "      <th>Fairness_Demographic_Parity_DNN_prediction_Training</th>\n",
       "      <th>Fairness_Demographic_Parity_DNN_prediction_Testing</th>\n",
       "      <th>Fairness_Demographic_Parity_After_CatBoost_prediction_Training</th>\n",
       "      <th>Fairness_Demographic_Parity_After_CatBoost_prediction_Testing</th>\n",
       "      <th>Fairness_Treatment_Equality_Before_CatBoost_prediction_Training</th>\n",
       "      <th>Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing</th>\n",
       "      <th>Fairness_Treatment_Equality_DNN_prediction_Training</th>\n",
       "      <th>Fairness_Treatment_Equality_DNN_prediction_Testing</th>\n",
       "      <th>Fairness_Treatment_Equality_After_CatBoost_prediction_Training</th>\n",
       "      <th>Fairness_Treatment_Equality_After_CatBoost_prediction_Testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Data, Dataset_name, Alpha-pred, Beta-Fair, Gamma-privacy, cost_multiplier-all, Balance_Class, Balance_Sensitive, Fairness_cost_function, Dataset_training_size, Dataset_testing_size, Test_loss_predictor, Test_loss_Discriminator_fool, Test_total_loss_gen, Train_loss_predictor, Train_loss_Discriminator_fool, Train_total_loss_gen, Dataset_fairness_Demographic_Parity, Report_DNN_Discriminator_Training_Accuracy, Report_DNN_Discriminator_Training_precision, Report_DNN_Discriminator_Training_recall, Report_DNN_Discriminator_Training_f1, Report_DNN_Discriminator_Testing_Accuracy, Report_DNN_Discriminator_Testing_precision, Report_DNN_Discriminator_Testing_recall, Report_DNN_Discriminator_Testing_f1, Report_Predictor_Training_Accuracy, Report_Predictor_Training_precision, Report_Predictor_Training_recall, Report_Predictor_Training_f1, Report_Predictor_Testing_Accuracy, Report_Predictor_Testing_precision, Report_Predictor_Testing_recall, Report_Predictor_Testing_f1, Report_Before_CatBoost_Prediction_Training_Accuracy, Report_Before_CatBoost_Prediction_Training_precision, Report_Before_CatBoost_Prediction_Training_recall, Report_Before_CatBoost_Prediction_Training_f1, Report_Before_CatBoost_Prediction_Testing_Accuracy, Report_Before_CatBoost_Prediction_Testing_precision, Report_Before_CatBoost_Prediction_Testing_recall, Report_Before_CatBoost_Prediction_Testing_f1, Report_After_CatBoost_Prediction_Training_Accuracy, Report_After_CatBoost_Prediction_Training_precision, Report_After_CatBoost_Prediction_Training_recall, Report_After_CatBoost_Prediction_Training_f1, Report_After_CatBoost_Prediction_Testing_Accuracy, Report_After_CatBoost_Prediction_Testing_precision, Report_After_CatBoost_Prediction_Testing_recall, Report_After_CatBoost_Prediction_Testing_f1, Report_Before_CatBoost_Sensitive_Training_Accuracy, Report_Before_CatBoost_Sensitive_Training_precision, Report_Before_CatBoost_Sensitive_Training_recall, Report_Before_CatBoost_Sensitive_Training_f1, Report_Before_CatBoost_Sensitive_Testing_Accuracy, Report_Before_CatBoost_Sensitive_Testing_precision, Report_Before_CatBoost_Sensitive_Testing_recall, Report_Before_CatBoost_Sensitive_Testing_f1, Report_After_CatBoost_Sensitive_Training_Accuracy, Report_After_CatBoost_Sensitive_Training_precision, Report_After_CatBoost_Sensitive_Training_recall, Report_After_CatBoost_Sensitive_Training_f1, Report_After_CatBoost_Sensitive_Testing_Accuracy, Report_After_CatBoost_Sensitive_Testing_precision, Report_After_CatBoost_Sensitive_Testing_recall, Report_After_CatBoost_Sensitive_Testing_f1, Fairness_Equal_Odd_Before_CatBoost_prediction_Training, Fairness_Equal_Odd_Before_CatBoost_prediction_Testing, Fairness_Equal_Odd_DNN_prediction_Training, Fairness_Equal_Odd_DNN_prediction_Testing, Fairness_Equal_Odd_After_CatBoost_prediction_Training, Fairness_Equal_Odd_After_CatBoost_prediction_Testing, Fairness_Equal_Opportunity_Before_CatBoost_prediction_Training, Fairness_Equal_Opportunity_Before_CatBoost_prediction_Testing, Fairness_Equal_Opportunity_DNN_prediction_Training, Fairness_Equal_Opportunity_DNN_prediction_Testing, Fairness_Equal_Opportunity_After_CatBoost_prediction_Training, Fairness_Equal_Opportunity_After_CatBoost_prediction_Testing, Fairness_Demographic_Parity_Before_CatBoost_prediction_Training, Fairness_Demographic_Parity_Before_CatBoost_prediction_Testing, Fairness_Demographic_Parity_DNN_prediction_Training, Fairness_Demographic_Parity_DNN_prediction_Testing, Fairness_Demographic_Parity_After_CatBoost_prediction_Training, Fairness_Demographic_Parity_After_CatBoost_prediction_Testing, Fairness_Treatment_Equality_Before_CatBoost_prediction_Training, Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing, Fairness_Treatment_Equality_DNN_prediction_Training, Fairness_Treatment_Equality_DNN_prediction_Testing, Fairness_Treatment_Equality_After_CatBoost_prediction_Training, Fairness_Treatment_Equality_After_CatBoost_prediction_Testing]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 90 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(columns=['Data', 'Dataset_name',\n",
    "        \"Alpha-pred\",\n",
    "        \"Beta-Fair\",\n",
    "        \"Gamma-privacy\",\n",
    "        \"cost_multiplier-all\", 'Balance_Class', 'Balance_Sensitive',\n",
    "       'Fairness_cost_function', 'Dataset_training_size',\n",
    "       'Dataset_testing_size', #'Fairness_cost_multiplier',\n",
    "        \"Test_loss_predictor\",\n",
    "        \"Test_loss_Discriminator_fool\",\n",
    "        \"Test_total_loss_gen\",\n",
    "        \"Train_loss_predictor\",\n",
    "        \"Train_loss_Discriminator_fool\",\n",
    "        \"Train_total_loss_gen\",\n",
    "       'Dataset_fairness_Demographic_Parity',\n",
    "       'Report_DNN_Discriminator_Training_Accuracy',\n",
    "       'Report_DNN_Discriminator_Training_precision',\n",
    "       'Report_DNN_Discriminator_Training_recall',\n",
    "       'Report_DNN_Discriminator_Training_f1',\n",
    "       'Report_DNN_Discriminator_Testing_Accuracy',\n",
    "       'Report_DNN_Discriminator_Testing_precision',\n",
    "       'Report_DNN_Discriminator_Testing_recall',\n",
    "       'Report_DNN_Discriminator_Testing_f1',\n",
    "       'Report_Predictor_Training_Accuracy',\n",
    "       'Report_Predictor_Training_precision',\n",
    "       'Report_Predictor_Training_recall', 'Report_Predictor_Training_f1',\n",
    "       'Report_Predictor_Testing_Accuracy',\n",
    "       'Report_Predictor_Testing_precision', 'Report_Predictor_Testing_recall',\n",
    "       'Report_Predictor_Testing_f1',\n",
    "       'Report_Before_CatBoost_Prediction_Training_Accuracy',\n",
    "       'Report_Before_CatBoost_Prediction_Training_precision',\n",
    "       'Report_Before_CatBoost_Prediction_Training_recall',\n",
    "       'Report_Before_CatBoost_Prediction_Training_f1',\n",
    "       'Report_Before_CatBoost_Prediction_Testing_Accuracy',\n",
    "       'Report_Before_CatBoost_Prediction_Testing_precision',\n",
    "       'Report_Before_CatBoost_Prediction_Testing_recall',\n",
    "       'Report_Before_CatBoost_Prediction_Testing_f1',\n",
    "       'Report_After_CatBoost_Prediction_Training_Accuracy',\n",
    "       'Report_After_CatBoost_Prediction_Training_precision',\n",
    "       'Report_After_CatBoost_Prediction_Training_recall',\n",
    "       'Report_After_CatBoost_Prediction_Training_f1',\n",
    "       'Report_After_CatBoost_Prediction_Testing_Accuracy',\n",
    "       'Report_After_CatBoost_Prediction_Testing_precision',\n",
    "       'Report_After_CatBoost_Prediction_Testing_recall',\n",
    "       'Report_After_CatBoost_Prediction_Testing_f1',\n",
    "       'Report_Before_CatBoost_Sensitive_Training_Accuracy',\n",
    "       'Report_Before_CatBoost_Sensitive_Training_precision',\n",
    "       'Report_Before_CatBoost_Sensitive_Training_recall',\n",
    "       'Report_Before_CatBoost_Sensitive_Training_f1',\n",
    "       'Report_Before_CatBoost_Sensitive_Testing_Accuracy',\n",
    "       'Report_Before_CatBoost_Sensitive_Testing_precision',\n",
    "       'Report_Before_CatBoost_Sensitive_Testing_recall',\n",
    "       'Report_Before_CatBoost_Sensitive_Testing_f1',\n",
    "       'Report_After_CatBoost_Sensitive_Training_Accuracy',\n",
    "       'Report_After_CatBoost_Sensitive_Training_precision',\n",
    "       'Report_After_CatBoost_Sensitive_Training_recall',\n",
    "       'Report_After_CatBoost_Sensitive_Training_f1',\n",
    "       'Report_After_CatBoost_Sensitive_Testing_Accuracy',\n",
    "       'Report_After_CatBoost_Sensitive_Testing_precision',\n",
    "       'Report_After_CatBoost_Sensitive_Testing_recall',\n",
    "       'Report_After_CatBoost_Sensitive_Testing_f1',\n",
    "       'Fairness_Equal_Odd_Before_CatBoost_prediction_Training',\n",
    "       'Fairness_Equal_Odd_Before_CatBoost_prediction_Testing',\n",
    "       'Fairness_Equal_Odd_DNN_prediction_Training',\n",
    "       'Fairness_Equal_Odd_DNN_prediction_Testing',\n",
    "       'Fairness_Equal_Odd_After_CatBoost_prediction_Training',\n",
    "       'Fairness_Equal_Odd_After_CatBoost_prediction_Testing',\n",
    "       'Fairness_Equal_Opportunity_Before_CatBoost_prediction_Training',\n",
    "       'Fairness_Equal_Opportunity_Before_CatBoost_prediction_Testing',\n",
    "       'Fairness_Equal_Opportunity_DNN_prediction_Training',\n",
    "       'Fairness_Equal_Opportunity_DNN_prediction_Testing',\n",
    "       'Fairness_Equal_Opportunity_After_CatBoost_prediction_Training',\n",
    "       'Fairness_Equal_Opportunity_After_CatBoost_prediction_Testing',\n",
    "       'Fairness_Demographic_Parity_Before_CatBoost_prediction_Training',\n",
    "       'Fairness_Demographic_Parity_Before_CatBoost_prediction_Testing',\n",
    "       'Fairness_Demographic_Parity_DNN_prediction_Training',\n",
    "       'Fairness_Demographic_Parity_DNN_prediction_Testing',\n",
    "       'Fairness_Demographic_Parity_After_CatBoost_prediction_Training',\n",
    "       'Fairness_Demographic_Parity_After_CatBoost_prediction_Testing',\n",
    "       'Fairness_Treatment_Equality_Before_CatBoost_prediction_Training',\n",
    "       'Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing',\n",
    "       'Fairness_Treatment_Equality_DNN_prediction_Training',\n",
    "       'Fairness_Treatment_Equality_DNN_prediction_Testing',\n",
    "       'Fairness_Treatment_Equality_After_CatBoost_prediction_Training',\n",
    "       'Fairness_Treatment_Equality_After_CatBoost_prediction_Testing'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1RDPCrYfC0f"
   },
   "source": [
    "# **Bootstrap Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-4aiKozcfRFJ"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_date = datetime.now()\n",
    "Data = current_date.strftime(\"%Y-%m-%d %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "editable": true,
    "id": "ofP9TYY8fEA_",
    "outputId": "60e0e1b9-4ce1-40e2-a070-c56bc56d5ec0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2091/3273344808.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sampled_df = pd.concat([sampled_df, sampled_group])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_count 1 count: 1 iter: <built-in function iter> Sensitive: 1 Class: 0  a:0.33 b:0.33 r:0.3399999999999999\n",
      "|."
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import seaborn as sns\n",
    "# Enable anomaly detection\n",
    "def remove_duplicate_columns(df):\n",
    "    df_transposed = df.T\n",
    "    df_unique = df_transposed.drop_duplicates(keep='first').T\n",
    "    return df_unique\n",
    "\n",
    "#=========================Demographic Parity Loss for training purposes=============================\n",
    "def demographic_parity_loss(predictor_pred, sen_batch):\n",
    "    # Calculate the probabilities of positive outcome for each group\n",
    "    positive_outcome_prob_group1 = predictor_pred[sen_batch == 0].mean()\n",
    "    positive_outcome_prob_group2 = predictor_pred[sen_batch == 1].mean()\n",
    "    # The loss is the absolute difference between these probabilities\n",
    "    dp_loss = abs(positive_outcome_prob_group1 - positive_outcome_prob_group2)\n",
    "\n",
    "    return dp_loss\n",
    "\n",
    "#########################################################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def update_hyperparameters(alpha, beta, gamma, loss_gen_to_help_pred, loss_gen_to_fool_dis, dp_loss_predictor, learning_rate, alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Updates the hyperparameters alpha, beta, and gamma using the Adam optimizer based on the losses from the latest model training iteration.\n",
    "\n",
    "    Parameters:\n",
    "    alpha (float): Current value of alpha.\n",
    "    beta (float): Current value of beta.\n",
    "    gamma (float): Current value of gamma.\n",
    "    loss_gen_to_help_pred (float): Latest loss associated with the predictor helping term.\n",
    "    loss_gen_to_fool_dis (float): Latest loss associated with the discriminator fooling term.\n",
    "    dp_loss_predictor (float): Latest loss associated with the fairness loss term for the predictor.\n",
    "    learning_rate (float): Learning rate for the hyperparameter updates.\n",
    "    alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v: Moving averages for Adam optimizer.\n",
    "    t (int): Timestep for Adam optimizer.\n",
    "    beta1, beta2 (float): Parameters for Adam optimizer.\n",
    "    epsilon (float): Small value to prevent division by zero in Adam.\n",
    "\n",
    "    # Initialize Adam parameters\n",
    "    alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t = 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Updated values of (alpha, beta, gamma) and their Adam parameters.\n",
    "    \"\"\"\n",
    "    # Calculate gradients based on inverse loss proportion\n",
    "    total_loss = loss_gen_to_help_pred + loss_gen_to_fool_dis + dp_loss_predictor\n",
    "    grad_alpha = -(loss_gen_to_help_pred / total_loss)\n",
    "    grad_beta = -(dp_loss_predictor / total_loss)\n",
    "    grad_gamma = -(loss_gen_to_fool_dis / total_loss)\n",
    "\n",
    "    # Update hyperparameters using Adam\n",
    "    def adam_update(param, grad, m, v, t):\n",
    "        t += 1\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        param -= learning_rate * m_hat / (np.sqrt(v_hat.detach().numpy()) + epsilon)\n",
    "        return param, m, v, t\n",
    "\n",
    "    alpha, alpha_m, alpha_v, t = adam_update(alpha, grad_alpha, alpha_m, alpha_v, t)\n",
    "    beta, beta_m, beta_v, t = adam_update(beta, grad_beta, beta_m, beta_v, t)\n",
    "    gamma, gamma_m, gamma_v, t = adam_update(gamma, grad_gamma, gamma_m, gamma_v, t)\n",
    "\n",
    "    return (alpha, beta, gamma, alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t)\n",
    "\n",
    "#########################################################################################################################################\n",
    "\n",
    "autograd.set_detect_anomaly(True)\n",
    "Dataset_name = \"German_Credit\"\n",
    "ds_count =0\n",
    "Balance_Sensitive = 1\n",
    "Balance_Class = 0\n",
    "\n",
    "\n",
    "ds_count+=1\n",
    "#------------------Balancing the dataset------------------------------------------------------------\n",
    "balanced_full_dataset = ohe_full_dataset.copy()\n",
    "\n",
    "if Balance_Sensitive==1 and Balance_Class==1:\n",
    "    balanced_full_dataset = equal_sampling(ohe_full_dataset, ['Sensitive','Class'])\n",
    "elif Balance_Sensitive==1 :\n",
    "    balanced_full_dataset = equal_sampling(ohe_full_dataset, ['Sensitive'])\n",
    "elif Balance_Class==1 :\n",
    "    balanced_full_dataset = equal_sampling(ohe_full_dataset, ['Class'])\n",
    "else:\n",
    "    balanced_full_dataset = ohe_full_dataset.loc[:,list(df_X.columns)+[\"Sensitive\",\"Class\"]]\n",
    "\n",
    "balanced_full_dataset = balanced_full_dataset.loc[:,list(df_X.columns)+[\"Sensitive\",\"Class\"]]\n",
    "balanced_full_dataset = balanced_full_dataset.dropna()\n",
    "balanced_full_dataset = remove_duplicate_columns(balanced_full_dataset)\n",
    "Dataset_fairness_Demographic_Parity = fair_Demographic_Parity(balanced_full_dataset,y_var='Class',sensitive_var='Sensitive',prediction='Class')\n",
    "\n",
    "# Selecting specific variables and dropping NA\n",
    "X_var = list(df_X.columns)\n",
    "Sen_var = df_sen.columns\n",
    "y_var = df_y.columns\n",
    "for i in X_var:\n",
    "    if i not in balanced_full_dataset.columns:\n",
    "        X_var.remove(i)\n",
    "\n",
    "# Splitting the dataset\n",
    "# X = balanced_full_dataset.loc[:, list(df_X.columns)].values\n",
    "X = balanced_full_dataset\n",
    "X.drop(['Class',\"Sensitive\"], axis=1)\n",
    "y = balanced_full_dataset.loc[:,  \"Class\"].values\n",
    "sen = balanced_full_dataset.loc[:, [\"Sensitive\"]].values\n",
    "X_var = list(X.columns)\n",
    "#trainspose the dataset\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "XT = scaler.transform(X)\n",
    "random_int = random.randint(0, 10000)\n",
    "# Assuming X, y, and z are your datasets\n",
    "X_train, X_test, y_train, y_test,sen_train, sen_test = train_test_split(XT, y,sen, test_size=0.3, random_state=random_int)\n",
    "X_train, X_test, y_train, y_test,sen_train, sen_test = X_train.astype(np.float32), X_test.astype(np.float32), y_train.astype(np.float32), y_test.astype(np.float32),sen_train.astype(np.float32), sen_test.astype(np.float32)\n",
    "y_train, y_test,sen_train, sen_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1),sen_train.reshape(-1, 1), sen_test.reshape(-1, 1)\n",
    "\n",
    "Dataset_training_size = len(y_train)\n",
    "Dataset_testing_size = len(y_test)\n",
    "\n",
    "count=0\n",
    "Alpha = 0.33\n",
    "Beta = 0.33\n",
    "Gamma = 1-Beta-Alpha\n",
    "time=[]\n",
    "while count<50:\n",
    "    count+=1\n",
    "    print(\"Dataset_count\",ds_count,\"count:\",str(count),\"iter:\",iter,\"Sensitive:\",\n",
    "          Balance_Sensitive,\"Class:\",Balance_Class,' a:'+str(Alpha)+' b:'+str(Beta)+' r:'+str(Gamma))\n",
    "\n",
    "    #=================================================== Model ====================================================\n",
    "    #########################################\n",
    "    # Attention Module (Feature-Attention)\n",
    "    #########################################\n",
    "    class FeatureAttention(nn.Module):\n",
    "        def __init__(self, dim, reduction=4):\n",
    "            super(FeatureAttention, self).__init__()\n",
    "            self.fc1 = nn.Linear(dim, dim // reduction)\n",
    "            self.fc2 = nn.Linear(dim // reduction, dim)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # x shape: (batch_size, dim)\n",
    "            att = self.fc1(x)\n",
    "            att = F.leaky_relu(att, 0.2)\n",
    "            att = self.fc2(att)\n",
    "            att = self.sigmoid(att)\n",
    "            return x * att\n",
    "    \n",
    "    #########################################\n",
    "    # Generator with Increased Depth, Neurons,\n",
    "    # Normalization, Dropout, Residual Connection, and Attention\n",
    "    #########################################\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Generator, self).__init__()\n",
    "            # Expanded layers with increased neurons\n",
    "            self.gen1 = nn.Linear(X_train.shape[1], 64)\n",
    "            self.ln1 = nn.LayerNorm(64)\n",
    "            \n",
    "            self.gen2 = nn.Linear(64, 128)\n",
    "            self.ln2 = nn.LayerNorm(128)\n",
    "            \n",
    "            self.gen3 = nn.Linear(128, 256)\n",
    "            self.ln3 = nn.LayerNorm(256)\n",
    "            \n",
    "            self.gen4 = nn.Linear(256, 256)\n",
    "            self.ln4 = nn.LayerNorm(256)\n",
    "            \n",
    "            self.gen5 = nn.Linear(256, 128)\n",
    "            self.ln5 = nn.LayerNorm(128)\n",
    "            \n",
    "            self.gen6 = nn.Linear(128, 64)\n",
    "            self.ln6 = nn.LayerNorm(64)\n",
    "            \n",
    "            self.gen7 = nn.Linear(64, X_train.shape[1])\n",
    "            \n",
    "            # Attention layer is applied after the last hidden block\n",
    "            self.attn = FeatureAttention(64)\n",
    "            \n",
    "            # Dropout for regularization\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = F.leaky_relu(self.ln1(self.gen1(x)), 0.2)\n",
    "            residual = x  # Save for residual connection (64-dim)\n",
    "            \n",
    "            x = F.leaky_relu(self.ln2(self.gen2(x)), 0.2)\n",
    "            x = self.dropout(x)\n",
    "            x = F.leaky_relu(self.ln3(self.gen3(x)), 0.2)\n",
    "            x = self.dropout(x)\n",
    "            x = F.leaky_relu(self.ln4(self.gen4(x)), 0.2)\n",
    "            x = self.dropout(x)\n",
    "            x = F.leaky_relu(self.ln5(self.gen5(x)), 0.2)\n",
    "            x = self.dropout(x)\n",
    "            x = F.leaky_relu(self.ln6(self.gen6(x)), 0.2)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            # Residual connection: add features from the first layer\n",
    "            x = x + residual\n",
    "            \n",
    "            # Apply attention to recalibrate features\n",
    "            x = self.attn(x)\n",
    "            \n",
    "            return torch.sigmoid(self.gen7(x))\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            # Discriminator\n",
    "            self.dis1 = nn.Linear(X_train.shape[1], 32)\n",
    "            self.dis2 = nn.Linear(32, 64)\n",
    "            self.dis3 = nn.Linear(64, 32)\n",
    "            self.dis4 = nn.Linear(32, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.leaky_relu(self.dis1(x), 0.1)\n",
    "            x = F.leaky_relu(self.dis2(x), 0.1)\n",
    "            x = F.leaky_relu(self.dis3(x), 0.1)\n",
    "            discriminator_output = F.sigmoid(self.dis4(x))\n",
    "            return discriminator_output\n",
    "\n",
    "    class Predictor(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Predictor, self).__init__()\n",
    "            # Predictor\n",
    "            self.pred1 = nn.Linear(X_train.shape[1], 32)\n",
    "            self.pred2 = nn.Linear(32, 64)\n",
    "            self.pred3 = nn.Linear(64, 128)\n",
    "            self.pred4 = nn.Linear(128, 64)\n",
    "            self.pred5 = nn.Linear(64, 32)\n",
    "            self.pred6 = nn.Linear(32, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.leaky_relu(self.pred1(x), 0.3)\n",
    "            x = F.leaky_relu(self.pred2(x), 0.3)\n",
    "            x = F.leaky_relu(self.pred3(x), 0.3)\n",
    "            x = F.leaky_relu(self.pred4(x), 0.3)\n",
    "            x = F.leaky_relu(self.pred5(x), 0.3)\n",
    "            prediction_output = F.sigmoid(self.pred6(x))\n",
    "\n",
    "            return prediction_output\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    #================== Create the training and testing dataloaders Train ==============================\n",
    "\n",
    "    # Converting to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    sen_tensor = torch.tensor(sen_train, dtype=torch.float32)\n",
    "\n",
    "    # Creating a TensorDataset\n",
    "    dataset = TensorDataset(X_tensor, y_tensor, sen_tensor)\n",
    "\n",
    "    # Create a DataLoader for batch processing\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    #================== Create the training and testing dataloaders Test ================================\n",
    "\n",
    "    # Converting to PyTorch tensors\n",
    "    X_Test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_Test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "    sen_Test_tensor = torch.tensor(sen_test, dtype=torch.float32)\n",
    "\n",
    "    # Creating a TensorDataset\n",
    "    dataset_Test = TensorDataset(X_Test_tensor, y_Test_tensor, sen_Test_tensor)\n",
    "\n",
    "    # Create a DataLoader for batch processing\n",
    "    Test_data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "\n",
    "    #=============================================================================================\n",
    "    #               Optimizer Expected values for accuracy, fairness and privacy\n",
    "    #==============================================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "    from catboost import CatBoostClassifier\n",
    "    \n",
    "    Before_catboost_model_class = CatBoostClassifier(border_count= 78, iterations= 30,\n",
    "                                        l2_leaf_reg= 1, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                        loss_function='CrossEntropy',verbose=0)\n",
    "\n",
    "    Before_catboost_model_class.fit(X_tensor.detach().numpy(), y_tensor.numpy())\n",
    "    class_labels = Before_catboost_model_class.predict(X_Test_tensor.detach().numpy())\n",
    "    report = classification_report(y_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Expected_predictor_accuracy =  df_report['precision']['accuracy']\n",
    "    Expected_predictor_fairness = 0.01\n",
    "    Expected_discriminator_accuracy = 0.50\n",
    "\n",
    "    #================================Model, loss function and optimizer Initiation======================\n",
    "    # Instantiate models\n",
    "    generator = Generator()\n",
    "    discriminator = Discriminator()\n",
    "    predictor = Predictor()\n",
    "\n",
    "    # Define loss functions and optimizers\n",
    "    criterion_fake_real = nn.BCELoss()\n",
    "    criterion_classification = nn.BCELoss()\n",
    "    criterion_prediction = nn.BCELoss()\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=0.001)\n",
    "    optimizer_dis = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "    optimizer_pred = optim.Adam(predictor.parameters(), lr=0.002)\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    #                                   Training Loop\n",
    "    ####################################################################################################\n",
    "    num_epochs = 50  # Number of epochs\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    Date = current_date.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch%10==0:\n",
    "            print(\"|\",end=\"\")\n",
    "        else:\n",
    "            print(\".\",end=\"\")\n",
    "        for batch in data_loader:\n",
    "            # Load batch data\n",
    "            X_batch, y_batch, sen_batch = batch\n",
    "\n",
    "            # Reset gradients for all optimizers\n",
    "            optimizer_gen.zero_grad()\n",
    "            optimizer_dis.zero_grad()\n",
    "            optimizer_pred.zero_grad()\n",
    "\n",
    "            # Forward pass through generator\n",
    "            gen_data = generator(X_batch)\n",
    "\n",
    "            # Discriminator Training\n",
    "            # Discriminator tries to predict sensitive attribute from generated data\n",
    "            discriminator_pred = discriminator(gen_data)\n",
    "            loss_discriminator = criterion_fake_real(discriminator_pred, sen_batch)\n",
    "            loss_discriminator.backward(retain_graph=True)  # Retain graph for generator's update\n",
    "            optimizer_dis.step()\n",
    "\n",
    "            # Predictor Training\n",
    "            # Predictor tries to predict target from generated data\n",
    "            predictor_pred = predictor(gen_data)\n",
    "            dp_loss_predictor = demographic_parity_loss(predictor_pred, sen_batch)\n",
    "            loss_predictor = criterion_prediction(predictor_pred, y_batch) + (dp_loss_predictor* Beta)\n",
    "            loss_predictor.backward(retain_graph=True)  # Retain graph for generator's update\n",
    "            optimizer_pred.step()\n",
    "\n",
    "            # Generator Training\n",
    "            discriminator_pred_for_gen = discriminator(gen_data)\n",
    "            loss_gen_to_fool_dis = criterion_fake_real(discriminator_pred_for_gen, 1 - sen_batch)# Generator tries to fool discriminator and help predictor\n",
    "            disc_accuracy = accuracy_score(np.round(discriminator_pred_for_gen.detach().numpy()), sen_batch)# Fooling discriminator\n",
    "\n",
    "            # Helping predictor\n",
    "            predictor_pred_for_gen = predictor(gen_data)\n",
    "            loss_gen_to_help_pred = criterion_classification(predictor_pred_for_gen, y_batch)\n",
    "            pred_accuracy = accuracy_score(np.round(predictor_pred_for_gen.detach().numpy()), y_batch)\n",
    "\n",
    "            # Combine losses for generator\n",
    "            dp_loss_predictor = demographic_parity_loss(predictor(generator(X_batch)), sen_batch)\n",
    "            total_loss_gen =   Alpha*loss_gen_to_help_pred*5 + Gamma*loss_gen_to_fool_dis + Beta*dp_loss_predictor\n",
    "            total_loss_gen.backward()\n",
    "            optimizer_gen.step()\n",
    "\n",
    "\n",
    "    Test_class_pred = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    Test_sen_pred = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    Test_pred_accuracy = accuracy_score(Test_class_pred, y_Test_tensor.numpy())\n",
    "    Test_dp_loss_predictor = demographic_parity_loss(Test_sen_pred, sen_Test_tensor.numpy())\n",
    "    Test_discriminator_pred = np.where(discriminator( generator(X_Test_tensor))>= 0.5, 1, 0)\n",
    "    Test_discriminator_accuracy = accuracy_score(Test_discriminator_pred, sen_Test_tensor.numpy())\n",
    "\n",
    "    Test_discriminator_pred_for_gen = discriminator(generator(X_Test_tensor))\n",
    "    Test_loss_gen_to_fool_dis = criterion_fake_real(Test_discriminator_pred_for_gen, 1 - sen_Test_tensor)\n",
    "    Test_loss_gen_to_help_pred = criterion_classification(predictor( generator(X_Test_tensor)), y_Test_tensor)\n",
    "    Test_dp_loss_predictor = demographic_parity_loss(predictor(generator(X_Test_tensor)), sen_Test_tensor)\n",
    "    Test_total_loss_gen =   Test_loss_gen_to_help_pred + Test_loss_gen_to_fool_dis + Test_dp_loss_predictor\n",
    "\n",
    "    Train_class_pred = np.where(predictor( generator(X_tensor)) >= 0.5, 1, 0)\n",
    "    Train_sen_pred = np.where(predictor( generator(X_tensor)) >= 0.5, 1, 0)\n",
    "    Train_pred_accuracy = accuracy_score(Train_class_pred, y_tensor.numpy())\n",
    "    Train_dp_loss_predictor = demographic_parity_loss(Train_sen_pred, sen_tensor.numpy())\n",
    "    Train_discriminator_pred = np.where(discriminator( generator(X_tensor))>= 0.5, 1, 0)\n",
    "    Train_discriminator_accuracy = accuracy_score(Train_discriminator_pred, sen_tensor.numpy())\n",
    "\n",
    "    Train_discriminator_pred_for_gen = discriminator(generator(X_tensor))\n",
    "    Train_loss_gen_to_fool_dis = criterion_fake_real(Train_discriminator_pred_for_gen, 1 - sen_tensor)\n",
    "    Train_loss_gen_to_help_pred = criterion_classification(predictor( generator(X_tensor)), y_tensor)\n",
    "    Train_dp_loss_predictor = demographic_parity_loss(predictor(generator(X_batch)), sen_batch)\n",
    "    Train_total_loss_gen =   Train_loss_gen_to_help_pred + Train_loss_gen_to_fool_dis + Train_dp_loss_predictor\n",
    "\n",
    "    current_time = datetime.now()\n",
    "    delta = current_time - start_time\n",
    "    time+=[delta]\n",
    "    \n",
    "\n",
    "    #=================================================================\n",
    "    #              Plot the prediction probabilities\n",
    "    #=================================================================\n",
    "    class_labels = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(y_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Predictor_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    #---------------------------------------------------------------------------------------------------------------------------\n",
    "    class_labels = np.where(discriminator( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(sen_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_DNN_Discriminator_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    #---------------------------------------------------------------------------------------------------------------------------\n",
    "    catboost_model = CatBoostClassifier(border_count= 78, depth= 5, iterations= 20,\n",
    "                                                            l2_leaf_reg= 3, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                                            loss_function='CrossEntropy',verbose=0)\n",
    "    catboost_model.fit(generator(X_tensor).detach().numpy(), sen_tensor.numpy())\n",
    "    class_labels = catboost_model.predict(generator(X_Test_tensor).detach().numpy())\n",
    "    report = classification_report(sen_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_After_CatBoost_Sensitive_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    #---------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    probabilities_class = np.array(predictor(generator(X_Test_tensor)).detach())[:,0]\n",
    "    probabilities_sex = np.array(discriminator(generator(X_Test_tensor)).detach())[:,0]\n",
    "    class_labels = catboost_model.predict_proba(generator(X_Test_tensor).detach().numpy())\n",
    "    probabilities_sex_catB = np.array(class_labels[:,1])\n",
    "    \n",
    "    # Define the bin width\n",
    "    bin_width = 0.02\n",
    "    \n",
    "    # Determine the global minimum and maximum to ensure all histograms use the same bins\n",
    "    min_prob = 0\n",
    "    max_prob = 1\n",
    "    \n",
    "    # Create bins from the minimum to the maximum value with the specified bin width\n",
    "    bins = np.arange(min_prob, max_prob + bin_width, bin_width)\n",
    "    \n",
    "    # Format accuracy values to three decimal points\n",
    "    formatted_class_accuracy = f\"{Report_Predictor_Testing_Accuracy:.3f}\"\n",
    "    formatted_gender_accuracy = f\"{Report_DNN_Discriminator_Testing_Accuracy:.3f}\"\n",
    "    formatted_gender_catb_accuracy = f\"{Report_After_CatBoost_Sensitive_Testing_Accuracy:.3f}\"\n",
    "    \n",
    "    # Plot histograms with formatted labels\n",
    "    plt.hist(probabilities_class, bins=bins, density=True, alpha=0.5, label=f'Class (Acc: {formatted_class_accuracy})')\n",
    "    plt.hist(probabilities_sex, bins=bins, density=True, alpha=0.5, label=f'Gender (Acc: {formatted_gender_accuracy})')\n",
    "    plt.hist(probabilities_sex_catB, bins=bins, density=True, alpha=0.5, label=f'Gender_CatB (Acc: {formatted_gender_catb_accuracy})')\n",
    "    \n",
    "    plt.legend(title='Probability Distributions')\n",
    "    plt.title('Density Plot of Prediction Probabilities at CM: '+' a:'+str(Alpha)+' b:'+str(Beta)+' r:'+str(Gamma))\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Density')\n",
    "    plt.yscale('log')  # Set the y-axis to logarithmic scale\n",
    "    plt.savefig('Plots_op_2/Density Plot of Prediction Probabilities at CM: ' + ' a:'+str(np.round(Alpha,2))+' b:'+str(np.round(Beta,2))+' r:'+str(np.round(Gamma,2))+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    from sklearn.metrics import log_loss\n",
    "    from catboost import CatBoostClassifier\n",
    "    catboost_model = CatBoostClassifier(border_count= 78, depth= 5, iterations= 20,\n",
    "                                        l2_leaf_reg= 3, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                        loss_function='CrossEntropy',verbose=0)\n",
    "\n",
    "    catboost_model.fit(generator(X_tensor).detach().numpy(), sen_tensor.numpy())\n",
    "    class_labels = catboost_model.predict(generator(X_tensor).detach().numpy())\n",
    "    class_labels = 1 - class_labels\n",
    "    cat_sen_bce_loss = log_loss(sen_tensor.numpy(), class_labels)\n",
    "    print('------------cat_dis_loss',cat_sen_bce_loss)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #=================================================================\n",
    "    #               Discriminator report Training\n",
    "    #=================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = np.where(discriminator( generator(X_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(sen_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_DNN_Discriminator_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_DNN_Discriminator_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_DNN_Discriminator_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_DNN_Discriminator_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Discriminator report Tessting\n",
    "    #=================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = np.where(discriminator( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(sen_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_DNN_Discriminator_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_DNN_Discriminator_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_DNN_Discriminator_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_DNN_Discriminator_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Predictor report Training\n",
    "    #=================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = np.where(predictor( generator(X_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(y_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Predictor_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Predictor_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_Predictor_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_Predictor_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Predictor report Testing\n",
    "    #=================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    report = classification_report(y_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Predictor_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Predictor_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_Predictor_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_Predictor_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "\n",
    "    #=================================================================\n",
    "    #               Before CatBoost Predictor report - Training\n",
    "    #=================================================================\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    from catboost import CatBoostClassifier\n",
    "    Before_catboost_model_class = CatBoostClassifier(border_count= 78, depth= 5, iterations= 20,\n",
    "                                        l2_leaf_reg= 3, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                        loss_function='CrossEntropy',verbose=0)\n",
    "\n",
    "    Before_catboost_model_class.fit(X_tensor.detach().numpy(), y_tensor.numpy())\n",
    "    print(\"Training Score:\",Before_catboost_model_class.score(X_tensor.detach().numpy(), y_tensor.numpy()))\n",
    "\n",
    "    class_labels = Before_catboost_model_class.predict(X_tensor.detach().numpy())\n",
    "    report = classification_report(y_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Before_CatBoost_Prediction_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Before_CatBoost_Prediction_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_Before_CatBoost_Prediction_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_Before_CatBoost_Prediction_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Before CatBoost Predictor report - Testing\n",
    "    #=================================================================\n",
    "    class_labels = Before_catboost_model_class.predict(X_Test_tensor.detach().numpy())\n",
    "    report = classification_report(y_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Before_CatBoost_Prediction_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Before_CatBoost_Prediction_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_Before_CatBoost_Prediction_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_Before_CatBoost_Prediction_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               After CatBoost Predictor report\n",
    "    #=================================================================\n",
    "    from catboost import CatBoostClassifier\n",
    "    After_catboost_model_class = CatBoostClassifier(border_count= 178, depth= 8, iterations= 20,\n",
    "                                        l2_leaf_reg= 1, learning_rate= 0.3, min_data_in_leaf= 5,\n",
    "                                        loss_function='CrossEntropy',verbose=0 )\n",
    "\n",
    "    After_catboost_model_class.fit(generator(X_tensor).detach().numpy(), y_tensor.numpy())\n",
    "    print(\"Training Score:\",After_catboost_model_class.score(generator(X_tensor).detach().numpy(), y_tensor.numpy()))\n",
    "\n",
    "    class_labels = After_catboost_model_class.predict(generator(X_tensor).detach().numpy())\n",
    "    report = classification_report(y_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_After_CatBoost_Prediction_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_After_CatBoost_Prediction_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_After_CatBoost_Prediction_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_After_CatBoost_Prediction_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Before CatBoost Predictor report - Testing\n",
    "    #=================================================================\n",
    "    class_labels = After_catboost_model_class.predict(generator(X_Test_tensor).detach().numpy())\n",
    "    report = classification_report(y_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_After_CatBoost_Prediction_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_After_CatBoost_Prediction_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_After_CatBoost_Prediction_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_After_CatBoost_Prediction_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #            Before CatBoost Sen_Var Predictor report - Training\n",
    "    #=================================================================\n",
    "    from catboost import CatBoostClassifier\n",
    "    catboost_model = CatBoostClassifier(border_count= 78, depth= 5, iterations= 20,\n",
    "                                        l2_leaf_reg= 3, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                        loss_function='CrossEntropy',verbose=0)\n",
    "\n",
    "    catboost_model.fit(X_tensor.detach().numpy(), sen_tensor.numpy())\n",
    "    print(\"Training Score:\",catboost_model.score(X_tensor.detach().numpy(), sen_tensor.numpy()))\n",
    "\n",
    "    class_labels = catboost_model.predict(X_tensor.detach().numpy())\n",
    "    report = classification_report(sen_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Before_CatBoost_Sensitive_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Before_CatBoost_Sensitive_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_Before_CatBoost_Sensitive_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_Before_CatBoost_Sensitive_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               Before CatBoost Sen_Var Predictor report - Testing\n",
    "    #=================================================================\n",
    "    class_labels = catboost_model.predict(X_Test_tensor.detach().numpy())\n",
    "    report = classification_report(sen_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_Before_CatBoost_Sensitive_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_Before_CatBoost_Sensitive_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_Before_CatBoost_Sensitive_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_Before_CatBoost_Sensitive_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #            After CatBoost Sen_Var Predictor report - Training\n",
    "    #=================================================================\n",
    "    from catboost import CatBoostClassifier\n",
    "    catboost_model = CatBoostClassifier(border_count= 78, depth= 5, iterations= 20,\n",
    "                                        l2_leaf_reg= 3, learning_rate= 0.2, min_data_in_leaf= 15,\n",
    "                                        loss_function='CrossEntropy',verbose=0)\n",
    "\n",
    "    catboost_model.fit(generator(X_tensor).detach().numpy(), sen_tensor.numpy())\n",
    "    print(\"Training Score:\",catboost_model.score(generator(X_tensor).detach().numpy(), sen_tensor.numpy()))\n",
    "\n",
    "    class_labels = catboost_model.predict(generator(X_tensor).detach().numpy())\n",
    "    report = classification_report(sen_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_After_CatBoost_Sensitive_Training_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_After_CatBoost_Sensitive_Training_precision = df_report['precision']['weighted avg']\n",
    "    Report_After_CatBoost_Sensitive_Training_recall = df_report['recall']['weighted avg']\n",
    "    Report_After_CatBoost_Sensitive_Training_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "    #=================================================================\n",
    "    #               After CatBoost Sen_Var Predictor report - Testing\n",
    "    #=================================================================\n",
    "    class_labels = catboost_model.predict(generator(X_Test_tensor).detach().numpy())\n",
    "    report = classification_report(sen_Test_tensor.numpy(), class_labels, target_names=['Class 0', 'Class 1'],output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    Report_After_CatBoost_Sensitive_Testing_Accuracy =  df_report['precision']['accuracy']\n",
    "    Report_After_CatBoost_Sensitive_Testing_precision = df_report['precision']['weighted avg']\n",
    "    Report_After_CatBoost_Sensitive_Testing_recall = df_report['recall']['weighted avg']\n",
    "    Report_After_CatBoost_Sensitive_Testing_f1 = df_report['f1-score']['weighted avg']\n",
    "\n",
    "\n",
    "    #=================================================================\n",
    "    #               Merge the data to a single data-frame\n",
    "    #=================================================================\n",
    "    Training_df = pd.DataFrame(X_train,columns=X_var)\n",
    "    Training_df[\"sen\"] = sen_train\n",
    "    Training_df[\"pred\"] = y_train\n",
    "\n",
    "    Training_df[\"After_prediction_DNN\"] = np.where(predictor( generator(X_tensor)) >= 0.5, 1, 0)\n",
    "    Training_df[\"After_prediction_CatB\"] = After_catboost_model_class.predict(generator(X_tensor).detach().numpy())\n",
    "    Training_df[\"Before_prediction_CatB\"] = Before_catboost_model_class.predict(X_tensor.detach().numpy())\n",
    "\n",
    "\n",
    "    Testing_df = pd.DataFrame(X_test,columns=X_var)\n",
    "    Testing_df[\"sen\"] = sen_test\n",
    "    Testing_df[\"pred\"] = y_test\n",
    "\n",
    "    Testing_df[\"After_prediction_DNN\"] = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    Testing_df[\"After_prediction_CatB\"] = After_catboost_model_class.predict(generator(X_Test_tensor).detach().numpy())\n",
    "    Testing_df[\"Before_prediction_CatB\"] = Before_catboost_model_class.predict(X_Test_tensor.detach().numpy())\n",
    "\n",
    "    #=================================================================\n",
    "    #              Plot the prediction probabilities\n",
    "    #=================================================================\n",
    "    probabilities_class = np.array(predictor(generator(X_Test_tensor)).detach())[:,0]\n",
    "    probabilities_sex = np.array(discriminator(generator(X_Test_tensor)).detach())[:,0]\n",
    "    class_labels = catboost_model.predict_proba(generator(X_Test_tensor).detach().numpy())\n",
    "    probabilities_sex_catB = np.array(class_labels[:,1])\n",
    "    \n",
    "    # Define the bin width\n",
    "    bin_width = 0.02\n",
    "    \n",
    "    # Determine the global minimum and maximum to ensure all histograms use the same bins\n",
    "    min_prob = min(probabilities_class.min(), probabilities_sex.min(), probabilities_sex_catB.min())\n",
    "    max_prob = max(probabilities_class.max(), probabilities_sex.max(), probabilities_sex_catB.max())\n",
    "    \n",
    "    # Create bins from the minimum to the maximum value with the specified bin width\n",
    "    bins = np.arange(min_prob, max_prob + bin_width, bin_width)\n",
    "    \n",
    "    # Format accuracy values to three decimal points\n",
    "    formatted_class_accuracy = f\"{Report_Predictor_Testing_Accuracy:.3f}\"\n",
    "    formatted_gender_accuracy = f\"{Report_DNN_Discriminator_Testing_Accuracy:.3f}\"\n",
    "    formatted_gender_catb_accuracy = f\"{Report_After_CatBoost_Sensitive_Testing_Accuracy:.3f}\"\n",
    "    \n",
    "    # Plot histograms with formatted labels\n",
    "    plt.hist(probabilities_class, bins=bins, density=True, alpha=0.5, label=f'Class (Acc: {formatted_class_accuracy})')\n",
    "    plt.hist(probabilities_sex, bins=bins, density=True, alpha=0.5, label=f'Gender (Acc: {formatted_gender_accuracy})')\n",
    "    plt.hist(probabilities_sex_catB, bins=bins, density=True, alpha=0.5, label=f'Gender_CatB (Acc: {formatted_gender_catb_accuracy})')\n",
    "    \n",
    "    plt.legend(title='Probability Distributions')\n",
    "    plt.title('Density Plot of Prediction Probabilities at CM: ' + ' a:'+str(Alpha)+' b:'+str(Beta)+' r:'+str(Gamma))\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Density')\n",
    "    # plt.yscale('log')  # Set the y-axis to logarithmic scale\n",
    "    plt.savefig('plots_op_4_GC/Density Plot of Prediction Probabilities at CM: ' + ' a:'+str(np.round(Alpha,2))+' b:'+str(np.round(Beta,2))+' r:'+str(np.round(Gamma,2))+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    continue\n",
    "\n",
    "    #=================================================================\n",
    "    #                  Equal Odd\n",
    "    #=================================================================\n",
    "    Fairness_Equal_Odd_Before_CatBoost_prediction_Training = fair_Eq_Odds(Training_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Equal_Odd_Before_CatBoost_prediction_Testing = fair_Eq_Odds(Testing_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Equal_Odd_DNN_prediction_Training = fair_Eq_Odds(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Equal_Odd_DNN_prediction_Testing = fair_Eq_Odds(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Equal_Odd_After_CatBoost_prediction_Training = fair_Eq_Odds(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Equal_Odd_After_CatBoost_prediction_Testing = fair_Eq_Odds(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "\n",
    "    #=================================================================\n",
    "    #                  Equal Opportunity\n",
    "    #=================================================================\n",
    "    Fairness_Equal_Opportunity_Before_CatBoost_prediction_Training = fair_Eq_Opportunity(Training_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Equal_Opportunity_Before_CatBoost_prediction_Testing = fair_Eq_Opportunity(Testing_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Equal_Opportunity_DNN_prediction_Training = fair_Eq_Opportunity(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Equal_Opportunity_DNN_prediction_Testing = fair_Eq_Opportunity(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Equal_Opportunity_After_CatBoost_prediction_Training = fair_Eq_Opportunity(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Equal_Opportunity_After_CatBoost_prediction_Testing = fair_Eq_Opportunity(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "\n",
    "    #=================================================================\n",
    "    #                  Demographic Parity\n",
    "    #=================================================================\n",
    "    Fairness_Demographic_Parity_Before_CatBoost_prediction_Training = fair_Demographic_Parity(Training_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Demographic_Parity_Before_CatBoost_prediction_Testing = fair_Demographic_Parity(Testing_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Demographic_Parity_DNN_prediction_Training = fair_Demographic_Parity(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Demographic_Parity_DNN_prediction_Testing = fair_Demographic_Parity(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Demographic_Parity_After_CatBoost_prediction_Training = fair_Demographic_Parity(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Demographic_Parity_After_CatBoost_prediction_Testing = fair_Demographic_Parity(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "\n",
    "    #=================================================================\n",
    "    #                  Treatment Equality\n",
    "    #=================================================================\n",
    "    Fairness_Treatment_Equality_Before_CatBoost_prediction_Training = fair_Treatment_Equality(Training_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing = fair_Treatment_Equality(Testing_df,y_var='pred',sensitive_var='sen',prediction='Before_prediction_CatB')\n",
    "    Fairness_Treatment_Equality_DNN_prediction_Training = fair_Treatment_Equality(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Treatment_Equality_DNN_prediction_Testing = fair_Treatment_Equality(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_DNN')\n",
    "    Fairness_Treatment_Equality_After_CatBoost_prediction_Training = fair_Treatment_Equality(Training_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "    Fairness_Treatment_Equality_After_CatBoost_prediction_Testing = fair_Treatment_Equality(Testing_df,y_var='pred',sensitive_var='sen',prediction='After_prediction_CatB')\n",
    "\n",
    "    #=================================================================\n",
    "    #                  create the results table\n",
    "    #=================================================================\n",
    "\n",
    "    current_date = datetime.now()\n",
    "    Date = current_date.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "\n",
    "    results = { \"Data\": [Date],\n",
    "    \"Dataset_name\": [Dataset_name],\n",
    "    \"Alpha-pred\": [Alpha],\n",
    "    \"Beta-Fair\": [Beta],\n",
    "    \"Gamma-privacy\": [Gamma],\n",
    "    # \"cost_multiplier-all\": [cost_multiplier],\n",
    "    \"Balance_Class\": [Balance_Class] ,\n",
    "    \"Balance_Sensitive\": [Balance_Sensitive],\n",
    "    \"Dataset_training_size\": [Dataset_training_size],\n",
    "    \"Dataset_testing_size\": [Dataset_testing_size],\n",
    "    # \"Fairness_cost_function\": [Fairness_cost_function],\n",
    "    # \"Fairness_cost_multiplier\": [Fairness_cost_multiplier],\n",
    "    \"Test_loss_predictor\":[Test_loss_gen_to_help_pred.detach().numpy()],\n",
    "    \"Test_loss_Discriminator_fool\": [Test_loss_gen_to_fool_dis.detach().numpy()],\n",
    "    \"Test_total_loss_gen\": [Test_total_loss_gen.detach().numpy()],\n",
    "    \"Train_loss_predictor\":[Train_loss_gen_to_help_pred.detach().numpy()],\n",
    "    \"Train_loss_Discriminator_fool\": [Train_loss_gen_to_fool_dis.detach().numpy()],\n",
    "    \"Train_total_loss_gen\": [Train_total_loss_gen.detach().numpy()],\n",
    "    \"Dataset_fairness_Demographic_Parity\": [Dataset_fairness_Demographic_Parity],\n",
    "    \"Report_DNN_Discriminator_Training_Accuracy\": [Report_DNN_Discriminator_Training_Accuracy],\n",
    "    \"Report_DNN_Discriminator_Training_precision\": [Report_DNN_Discriminator_Training_precision],\n",
    "    \"Report_DNN_Discriminator_Training_recall\": [Report_DNN_Discriminator_Training_recall],\n",
    "    \"Report_DNN_Discriminator_Training_f1\": [Report_DNN_Discriminator_Training_f1],\n",
    "    \"Report_DNN_Discriminator_Testing_Accuracy\": [Report_DNN_Discriminator_Testing_Accuracy],\n",
    "    \"Report_DNN_Discriminator_Testing_precision\": [Report_DNN_Discriminator_Testing_precision],\n",
    "    \"Report_DNN_Discriminator_Testing_recall\": [Report_DNN_Discriminator_Testing_recall],\n",
    "    \"Report_DNN_Discriminator_Testing_f1\": [Report_DNN_Discriminator_Testing_f1],\n",
    "    \"Report_Predictor_Training_Accuracy\": [Report_Predictor_Training_Accuracy],\n",
    "    \"Report_Predictor_Training_precision\": [Report_Predictor_Training_precision],\n",
    "    \"Report_Predictor_Training_recall\": [Report_Predictor_Training_recall],\n",
    "    \"Report_Predictor_Training_f1\": [Report_Predictor_Training_f1],\n",
    "    \"Report_Predictor_Testing_Accuracy\": [Report_Predictor_Testing_Accuracy],\n",
    "    \"Report_Predictor_Testing_precision\": [Report_Predictor_Testing_precision],\n",
    "    \"Report_Predictor_Testing_recall\": [Report_Predictor_Testing_recall],\n",
    "    \"Report_Predictor_Testing_f1\": [Report_Predictor_Testing_f1],\n",
    "    \"Report_Before_CatBoost_Prediction_Training_Accuracy\": [Report_Before_CatBoost_Prediction_Training_Accuracy],\n",
    "    \"Report_Before_CatBoost_Prediction_Training_precision\": [Report_Before_CatBoost_Prediction_Training_precision],\n",
    "    \"Report_Before_CatBoost_Prediction_Training_recall\": [Report_Before_CatBoost_Prediction_Training_recall],\n",
    "    \"Report_Before_CatBoost_Prediction_Training_f1\": [Report_Before_CatBoost_Prediction_Training_f1],\n",
    "    \"Report_Before_CatBoost_Prediction_Testing_Accuracy\": [Report_Before_CatBoost_Prediction_Testing_Accuracy],\n",
    "    \"Report_Before_CatBoost_Prediction_Testing_precision\": [Report_Before_CatBoost_Prediction_Testing_precision],\n",
    "    \"Report_Before_CatBoost_Prediction_Testing_recall\": [Report_Before_CatBoost_Prediction_Testing_recall],\n",
    "    \"Report_Before_CatBoost_Prediction_Testing_f1\": [Report_Before_CatBoost_Prediction_Testing_f1],\n",
    "    \"Report_After_CatBoost_Prediction_Training_Accuracy\": [Report_After_CatBoost_Prediction_Training_Accuracy],\n",
    "    \"Report_After_CatBoost_Prediction_Training_precision\": [Report_After_CatBoost_Prediction_Training_precision],\n",
    "    \"Report_After_CatBoost_Prediction_Training_recall\": [Report_After_CatBoost_Prediction_Training_recall],\n",
    "    \"Report_After_CatBoost_Prediction_Training_f1\": [Report_After_CatBoost_Prediction_Training_f1],\n",
    "    \"Report_After_CatBoost_Prediction_Testing_Accuracy\": [Report_After_CatBoost_Prediction_Testing_Accuracy],\n",
    "    \"Report_After_CatBoost_Prediction_Testing_precision\": [Report_After_CatBoost_Prediction_Testing_precision],\n",
    "    \"Report_After_CatBoost_Prediction_Testing_recall\": [Report_After_CatBoost_Prediction_Testing_recall],\n",
    "    \"Report_After_CatBoost_Prediction_Testing_f1\": [Report_After_CatBoost_Prediction_Testing_f1],\n",
    "    \"Report_Before_CatBoost_Sensitive_Training_Accuracy\": [Report_Before_CatBoost_Sensitive_Training_Accuracy],\n",
    "    \"Report_Before_CatBoost_Sensitive_Training_precision\": [Report_Before_CatBoost_Sensitive_Training_precision],\n",
    "    \"Report_Before_CatBoost_Sensitive_Training_recall\": [Report_Before_CatBoost_Sensitive_Training_recall],\n",
    "    \"Report_Before_CatBoost_Sensitive_Training_f1\": [Report_Before_CatBoost_Sensitive_Training_f1],\n",
    "    \"Report_Before_CatBoost_Sensitive_Testing_Accuracy\": [Report_Before_CatBoost_Sensitive_Testing_Accuracy],\n",
    "    \"Report_Before_CatBoost_Sensitive_Testing_precision\": [Report_Before_CatBoost_Sensitive_Testing_precision],\n",
    "    \"Report_Before_CatBoost_Sensitive_Testing_recall\": [Report_Before_CatBoost_Sensitive_Testing_recall],\n",
    "    \"Report_Before_CatBoost_Sensitive_Testing_f1\": [Report_Before_CatBoost_Sensitive_Testing_f1],\n",
    "    \"Report_After_CatBoost_Sensitive_Training_Accuracy\": [Report_After_CatBoost_Sensitive_Training_Accuracy],\n",
    "    \"Report_After_CatBoost_Sensitive_Training_precision\":[ Report_After_CatBoost_Sensitive_Training_precision],\n",
    "    \"Report_After_CatBoost_Sensitive_Training_recall\": [Report_After_CatBoost_Sensitive_Training_recall],\n",
    "    \"Report_After_CatBoost_Sensitive_Training_f1\": [Report_After_CatBoost_Sensitive_Training_f1],\n",
    "    \"Report_After_CatBoost_Sensitive_Testing_Accuracy\": [Report_After_CatBoost_Sensitive_Testing_Accuracy],\n",
    "    \"Report_After_CatBoost_Sensitive_Testing_precision\": [Report_After_CatBoost_Sensitive_Testing_precision],\n",
    "    \"Report_After_CatBoost_Sensitive_Testing_recall\": [Report_After_CatBoost_Sensitive_Testing_recall],\n",
    "    \"Report_After_CatBoost_Sensitive_Testing_f1\": [Report_After_CatBoost_Sensitive_Testing_f1],\n",
    "    \"Fairness_Equal_Odd_Before_CatBoost_prediction_Training\": [Fairness_Equal_Odd_Before_CatBoost_prediction_Training],\n",
    "    \"Fairness_Equal_Odd_Before_CatBoost_prediction_Testing\": [Fairness_Equal_Odd_Before_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Equal_Odd_DNN_prediction_Training\": [Fairness_Equal_Odd_DNN_prediction_Training],\n",
    "    \"Fairness_Equal_Odd_DNN_prediction_Testing\": [Fairness_Equal_Odd_DNN_prediction_Testing],\n",
    "    \"Fairness_Equal_Odd_After_CatBoost_prediction_Training\": [Fairness_Equal_Odd_After_CatBoost_prediction_Training],\n",
    "    \"Fairness_Equal_Odd_After_CatBoost_prediction_Testing\": [Fairness_Equal_Odd_After_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Equal_Opportunity_Before_CatBoost_prediction_Training\": [Fairness_Equal_Opportunity_Before_CatBoost_prediction_Training],\n",
    "    \"Fairness_Equal_Opportunity_Before_CatBoost_prediction_Testing\": [Fairness_Equal_Opportunity_Before_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Equal_Opportunity_DNN_prediction_Training\": [Fairness_Equal_Opportunity_DNN_prediction_Training],\n",
    "    \"Fairness_Equal_Opportunity_DNN_prediction_Testing\": [Fairness_Equal_Opportunity_DNN_prediction_Testing],\n",
    "    \"Fairness_Equal_Opportunity_After_CatBoost_prediction_Training\": [Fairness_Equal_Opportunity_After_CatBoost_prediction_Training],\n",
    "    \"Fairness_Equal_Opportunity_After_CatBoost_prediction_Testing\": [Fairness_Equal_Opportunity_After_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Demographic_Parity_Before_CatBoost_prediction_Training\": [Fairness_Demographic_Parity_Before_CatBoost_prediction_Training],\n",
    "    \"Fairness_Demographic_Parity_Before_CatBoost_prediction_Testing\": [Fairness_Demographic_Parity_Before_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Demographic_Parity_DNN_prediction_Training\": [Fairness_Demographic_Parity_DNN_prediction_Training],\n",
    "    \"Fairness_Demographic_Parity_DNN_prediction_Testing\": [Fairness_Demographic_Parity_DNN_prediction_Testing],\n",
    "    \"Fairness_Demographic_Parity_After_CatBoost_prediction_Training\": [Fairness_Demographic_Parity_After_CatBoost_prediction_Training],\n",
    "    \"Fairness_Demographic_Parity_After_CatBoost_prediction_Testing\": [Fairness_Demographic_Parity_After_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Treatment_Equality_Before_CatBoost_prediction_Training\": [Fairness_Treatment_Equality_Before_CatBoost_prediction_Training],\n",
    "    \"Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing\": [Fairness_Treatment_Equality_Before_CatBoost_prediction_Testing],\n",
    "    \"Fairness_Treatment_Equality_DNN_prediction_Training\": [Fairness_Treatment_Equality_DNN_prediction_Training],\n",
    "    \"Fairness_Treatment_Equality_DNN_prediction_Testing\": [Fairness_Treatment_Equality_DNN_prediction_Testing],\n",
    "    \"Fairness_Treatment_Equality_After_CatBoost_prediction_Training\": [Fairness_Treatment_Equality_After_CatBoost_prediction_Training],\n",
    "    \"Fairness_Treatment_Equality_After_CatBoost_prediction_Testing\": [Fairness_Treatment_Equality_After_CatBoost_prediction_Testing],\n",
    "        }\n",
    "\n",
    "    df_temp_results = pd.DataFrame(results)\n",
    "    df_results = pd.concat([df_results, df_temp_results], ignore_index=True)\n",
    "    df_results.to_csv(Dataset_name+\"_\"+Date[:10]+\"_abl_a0.csv\")\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    #                                  Update Alpha, Beta, Gamma \n",
    "    ####################################################################################################\n",
    "    # Call the function after a training iteration\n",
    "    learning_rate = 1.5\n",
    "    # Assuming initial values for Alpha, Beta, Gamma and their respective Adam optimizer parameters\n",
    "    alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t = 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # When calling the update_hyperparameters function\n",
    "    Alpha, Beta, Gamma, alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t = update_hyperparameters(\n",
    "        Alpha, Beta, Gamma, loss_gen_to_help_pred*100, cat_sen_bce_loss, dp_loss_predictor, learning_rate,\n",
    "        alpha_m, alpha_v, beta_m, beta_v, gamma_m, gamma_v, t)\n",
    "\n",
    "    Alpha, Beta, Gamma = float(Alpha.detach().numpy()),   float(Beta.detach().numpy()),    float(Gamma.detach().numpy()) \n",
    "    print(f\" {count} Updated Alpha: {Alpha}, Beta: {Beta}, Gamma: {Gamma}\")\n",
    "\n",
    "    class_pred = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    sen_pred = np.where(predictor( generator(X_Test_tensor)) >= 0.5, 1, 0)\n",
    "    pred_accuracy = accuracy_score(class_pred, y_Test_tensor.numpy())\n",
    "    dp_loss_predictor = demographic_parity_loss(sen_pred, sen_Test_tensor.numpy())\n",
    "\n",
    "    # Alpha, Beta, Gamma = float(Alpha.detach().numpy(), Beta.detach().numpy(), Gamma.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_full_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgnXYbjfBl4c"
   },
   "outputs": [],
   "source": [
    "df_results.to_csv(Dataset_name+\"_\"+Date[:10]+\"_BF_10-5.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
